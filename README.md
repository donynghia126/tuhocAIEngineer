# Lá»™ TrÃ¬nh AI Engineer Full-Stack

Theo dÃµi chi tiáº¿t tá»«ng ngÃ y: LÃ½ thuyáº¿t, Thá»±c hÃ nh & Dá»± Ã¡n

## Giai Ä‘oáº¡n 1: ToÃ¡n â€“ Python â€“ TÆ° duy láº­p trÃ¬nh

**Thá»i gian:** 6 Tuáº§n | Táº­p trung 80% Thá»±c hÃ nh
**Má»¥c tiÃªu:** Náº¯m vá»¯ng Python, tÆ° duy thuáº­t toÃ¡n vÃ  toÃ¡n há»c ná»n táº£ng cho AI (Vector, Ma tráº­n).
**CÃ´ng cá»¥:** VS Code, Git, Python 3.x

### Tuáº§n 1: Python CÆ¡ Báº£n & Setup

_Viáº¿t Ä‘Æ°á»£c script cháº¡y logic cÆ¡ báº£n, quáº£n lÃ½ code báº±ng Git._

#### NgÃ y 1

- âœ… **LEARN**: CÃ i Ä‘áº·t VS Code, Python, Git. Biáº¿n (int, float, str), Kiá»ƒu dá»¯ liá»‡u.
- âœ… **CODE**: Viáº¿t chÆ°Æ¡ng trÃ¬nh tÃ­nh chá»‰ sá»‘ BMI, Ä‘á»•i nhiá»‡t Ä‘á»™ C sang F.

#### NgÃ y 2

- âœ… **LEARN**: CÃ¢u lá»‡nh Ä‘iá»u kiá»‡n (if-else), VÃ²ng láº·p (for, while).
- âœ… **CODE**: Giáº£i phÆ°Æ¡ng trÃ¬nh báº­c 2. Game Ä‘oÃ¡n sá»‘ (Random number guessing).

#### NgÃ y 3

- âœ… **LEARN**: List, Tuple, Set. CÃ¡c method xá»­ lÃ½ List.
- âœ… **CODE**: TÃ¬m sá»‘ lá»›n nháº¥t, sáº¯p xáº¿p list, xÃ³a pháº§n tá»­ trÃ¹ng láº·p.

#### NgÃ y 4

- âœ… **LEARN**: Dictionary (Tá»« Ä‘iá»ƒn) - Cáº¥u trÃºc quan trá»ng nháº¥t.
- âœ… **CODE**: Táº¡o tá»« Ä‘iá»ƒn Anh-Viá»‡t. Äáº¿m sá»‘ láº§n xuáº¥t hiá»‡n tá»« trong Ä‘oáº¡n vÄƒn.

#### NgÃ y 5

- âœ… **LEARN**: HÃ m (Function), Tham sá»‘, Return.
- âœ… **CODE**: Refactor bÃ i táº­p tuáº§n trÆ°á»›c thÃ nh cÃ¡c hÃ m riÃªng biá»‡t.

#### NgÃ y 6

- âœ… **LEARN**: Git cÆ¡ báº£n: git init, add, commit, push.
- âœ… **CODE**: Táº¡o repo GitHub vÃ  Ä‘áº©y toÃ n bá»™ bÃ i táº­p tuáº§n nÃ y lÃªn.

### Tuáº§n 2: Python NÃ¢ng cao & DSA

_Hiá»ƒu cÃ¡ch xá»­ lÃ½ dá»¯ liá»‡u phá»©c táº¡p vÃ  tá»‘i Æ°u code._

#### NgÃ y 7

- âœ… **LEARN**: List Comprehension, Lambda functions.
- âœ… **CODE**: Viáº¿t láº¡i cÃ¡c vÃ²ng láº·p xá»­ lÃ½ list chá»‰ báº±ng 1 dÃ²ng code.

#### NgÃ y 8

- âœ… **LEARN**: Xá»­ lÃ½ chuá»—i nÃ¢ng cao: split, join, strip, replace.
- âœ… **CODE**: BÃ i táº­p chuáº©n hÃ³a tÃªn ngÆ°á»i dÃ¹ng (viáº¿t hoa, xÃ³a khoáº£ng tráº¯ng).

#### NgÃ y 9

- âœ… **LEARN**: File Handling (Äá»c/Ghi file .txt).
- âœ… **CODE**: Äá»c file log vÃ  lá»c cÃ¡c dÃ²ng bÃ¡o lá»—i 'ERROR'.

#### NgÃ y 10

- âœ… **LEARN**: MÃ´i trÆ°á»ng áº£o (Virtual Environments)
- âœ… **CODE**: DÃ¹ng venv lÃ m Tool Web, DÃ¹ng conda giáº£ láº­p mÃ´i trÆ°á»ng cÅ©

#### NgÃ y 11

- âœ… **LEARN**: Python Package Manager (Pip & Conda)
- âœ… **CODE**: BÃ i táº­p 1 (Terminal): Táº¡o mÃ´i trÆ°á»ng áº£o má»›i báº±ng conda (Python 3.9). CÃ i Ä‘áº·t thÆ° viá»‡n pandas vÃ  matplotlib. Xuáº¥t danh sÃ¡ch thÆ° viá»‡n ra file environment.yml (chuáº©n Conda) hoáº·c requirements.txt (chuáº©n Pip).
- âœ… **CODE**: BÃ i táº­p 2 (Simulation): XÃ³a mÃ´i trÆ°á»ng vá»«a táº¡o. DÃ¹ng lá»‡nh Ä‘á»ƒ khÃ´i phá»¥c láº¡i toÃ n bá»™ mÃ´i trÆ°á»ng chá»‰ tá»« file cáº¥u hÃ¬nh vá»«a xuáº¥t.

#### NgÃ y 12

- âœ… **LEARN**: Python Type Errors - CÃ¡c lá»—i phá»• biáº¿n trong Python
- âœ… **CODE**: BÃ i táº­p (Bug Hunting): Cháº¡y code Ä‘á»ƒ xem Python bÃ¡o lá»—i gÃ¬ (IndexError, KeyError, TypeError, IndentationError, NameError). Giáº£i thÃ­ch táº¡i sao lá»—i. Sá»­a láº¡i cho cháº¡y Ä‘Ãºng.

#### NgÃ y 13

- âœ… **LEARN**: Exception Handling (Try-Except)
- âœ… **CODE**: MÃ¡y tÃ­nh an toÃ n, truy xuáº¥t dá»¯ liá»‡u an toÃ n

#### NgÃ y 14

- âœ… **LEARN**: Regular Expressions (Regex)
- âœ… **CODE**: TrÃ­ch xuáº¥t Email & SÄT, LÃ m sáº¡ch Password

### Tuáº§n 3: Python HÆ°á»›ng Ä‘á»‘i tÆ°á»£ng (OOP) & Advanced

_Chuyá»ƒn tÆ° duy tá»« viáº¿t hÃ m rá»i ráº¡c sang kiáº¿n trÃºc Class (ráº¥t quan trá»ng cho PyTorch sau nÃ y)._

#### NgÃ y 15

- âœ… **LEARN**: OOP Basic: Class & Object: Hiá»ƒu Class, Object, self, HÃ m **init** constructor
- âœ… **CODE**: Táº¡o class Student vá»›i cÃ¡c thuá»™c tÃ­nh name, age, grades

#### NgÃ y 16

- âœ… **LEARN**: OOP: Inheritance (Káº¿ thá»«a) & Polymorphism: Táº¡o class con káº¿ thá»«a class cha, Override phÆ°Æ¡ng thá»©c
- âœ… **CODE**: Táº¡o class AI_Student káº¿ thá»«a Student, thÃªm thuá»™c tÃ­nh 'specialization'

#### NgÃ y 17

- âœ… **LEARN**: OOP: Encapsulation (ÄÃ³ng gÃ³i): Private variables (\_\_var), Getter & Setter methods
- âœ… **CODE**: Báº£o máº­t Ä‘iá»ƒm sá»‘ sinh viÃªn, chá»‰ cho sá»­a qua hÃ m setter cÃ³ validation
- âœ… **LEARN**: Trá»«u tÆ°á»£ng (Abstraction) & Äa hÃ¬nh (Polymorphism)
- âœ… **CODE**: "Sá»Ÿ thÃº AI" (AI Model Zoo)

#### NgÃ y 18

- ğŸ“˜ **LEARN**: Python Advanced: Decorators: Hiá»ƒu First-class functions,Higher-order Function, Viáº¿t Wrapper function, decorator
- ğŸ’» **CODE**: Higher-Order Function(BÆ°á»›c Ä‘á»‡m),Wrapper Function (CÆ¡ cháº¿ lÃµi),Decorator @debug (Thá»­ thÃ¡ch)
- ğŸ’» **CODE**: Viáº¿t decorator @timer Ä‘á»ƒ Ä‘o thá»i gian cháº¡y cá»§a má»™t hÃ m (Quan trá»ng Ä‘á»ƒ tá»‘i Æ°u code AI)

#### NgÃ y 19

- ğŸ“˜ **LEARN**: Python Advanced: Generators & Iterators: yield keyword vs return, Xá»­ lÃ½ dá»¯ liá»‡u lá»›n báº±ng Generator
- ğŸ’» **CODE**: Viáº¿t hÃ m sinh sá»‘ Fibonacci vÃ´ háº¡n báº±ng yield

#### NgÃ y 20

- ğŸ“˜ **LEARN**: Exception Handling & Modules: try-except-finally custom, Tá»• chá»©c code thÃ nh modules/packages
- ğŸ’» **CODE**: NÃ¢ng cáº¥p bÃ i Student Manager: Xá»­ lÃ½ lá»—i nháº­p liá»‡u, tÃ¡ch file main.py vÃ  student.py

#### NgÃ y 21

- ğŸ“˜ **LEARN**: Review & Project OOP
- ğŸ’» **CODE**: Project: Upgrade CLI Student Manager. Chuyá»ƒn toÃ n bá»™ code ngÃ y 1-10 sang dáº¡ng Class. LÆ°u data vÃ o JSON báº±ng method cá»§a Class.

### Tuáº§n 4: Äáº¡i sá»‘ tuyáº¿n tÃ­nh (Linear Algebra) & Code Ma tráº­n

_Hiá»ƒu cÃ¡ch mÃ¡y tÃ­nh nhÃ¬n dá»¯ liá»‡u (dÆ°á»›i dáº¡ng Vector vÃ  Matrix)._

#### NgÃ y 22

- ğŸ“˜ **LEARN**: Scalar, Vector & Matrix: KhÃ¡i niá»‡m khÃ´ng gian vector, Biá»ƒu diá»…n áº£nh dÆ°á»›i dáº¡ng ma tráº­n
- ğŸ’» **CODE**: DÃ¹ng Python List of Lists Ä‘á»ƒ biá»ƒu diá»…n Ma tráº­n 2x2, 3x3

#### NgÃ y 23

- ğŸ“˜ **LEARN**: CÃ¡c phÃ©p toÃ¡n trÃªn Ma tráº­n (Cá»™ng, Trá»«, Scalar Multiply): Element-wise operations
- ğŸ’» **CODE**: Viáº¿t hÃ m matrix_add(A, B) vÃ  matrix_scalar_mul(A, k) khÃ´ng dÃ¹ng thÆ° viá»‡n

#### NgÃ y 24

- ğŸ“˜ **LEARN**: Dot Product (TÃ­ch vÃ´ hÆ°á»›ng) - QUAN TRá»ŒNG NHáº¤T: CÃ´ng thá»©c nhÃ¢n dÃ²ng x cá»™t, Ã nghÄ©a hÃ¬nh há»c (gÃ³c giá»¯a 2 vector)
- ğŸ“˜ **LEARN**: Norm (L1/L2), Distance & Cosine Similarity: liÃªn há»‡ vá»›i Dot Product vÃ  á»©ng dá»¥ng trong similarity/search.
- ğŸ’» **CODE**: Viáº¿t hÃ m dot_product(v1, v2) thá»§ cÃ´ng

#### NgÃ y 25

- ğŸ“˜ **LEARN**: Matrix Multiplication (NhÃ¢n ma tráº­n): Äiá»u kiá»‡n nhÃ¢n (mxn \* nxp), Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n
- ğŸ’» **CODE**: Viáº¿t hÃ m matrix_multiply(A, B) sá»­ dá»¥ng 3 vÃ²ng for lá»“ng nhau

#### NgÃ y 26

- ğŸ“˜ **LEARN**: Transposition (Chuyá»ƒn vá»‹) & Reshape: Ma tráº­n chuyá»ƒn vá»‹ A.T, Thay Ä‘á»•i hÃ¬nh dáº¡ng ma tráº­n (Flatten)
- ğŸ“˜ **LEARN**: Vector Space, Basis & Orthonormal; Projection (chiáº¿u vector) â€“ trá»±c giÃ¡c dÃ¹ng trong least squares/PCA.
- ğŸ’» **CODE**: Viáº¿t hÃ m transpose(matrix)

#### NgÃ y 27

- ğŸ“˜ **LEARN**: Broadcasting (KhÃ¡i niá»‡m): CÃ¡ch NumPy cá»™ng 1 vector vÃ o 1 ma tráº­n (lÃ½ thuyáº¿t trÆ°á»›c khi dÃ¹ng thÆ° viá»‡n)
- ğŸ“˜ **LEARN**: Eigenvalues/Eigenvectors (trá»±c giÃ¡c): vÃ¬ sao 'hÆ°á»›ng riÃªng' quan trá»ng trong á»•n Ä‘á»‹nh, PCA, vÃ  dynamics cá»§a gradient.
- ğŸ’» **CODE**: MÃ´ phá»ng broadcasting báº±ng Python thuáº§n

#### NgÃ y 28

- ğŸ“˜ **LEARN**: Project: Mini Numpy Part 1
- ğŸ“˜ **LEARN**: SVD & PCA (trá»±c giÃ¡c): giáº£m chiá»u, nÃ©n thÃ´ng tin; liÃªn há»‡ embeddings vÃ  curse of dimensionality.
- ğŸ’» **CODE**: Project: MyLinearAlgebra Library. GÃ³i cÃ¡c hÃ m dot, multiply, transpose vÃ o Class 'MyMatrix'. Test vá»›i dá»¯ liá»‡u máº«u.

### Tuáº§n 5: Giáº£i tÃ­ch (Calculus) & XÃ¡c suáº¥t cÆ¡ báº£n

_Hiá»ƒu Gradient Descent hoáº¡t Ä‘á»™ng tháº¿ nÃ o (Äáº¡o hÃ m)._

#### NgÃ y 29

- ğŸ“˜ **LEARN**: Äáº¡o hÃ m (Derivative) cÆ¡ báº£n: Tá»‘c Ä‘á»™ thay Ä‘á»•i tá»©c thá»i, Quy táº¯c chuá»—i (Chain rule - Cá»‘t lÃµi cá»§a Backpropagation)
- ğŸ’» **CODE**: Viáº¿t hÃ m tÃ­nh Ä‘áº¡o hÃ m x^2 táº¡i Ä‘iá»ƒm x=3 (numerical differentiation)

#### NgÃ y 30

- ğŸ“˜ **LEARN**: Gradient & Partial Derivative (Äáº¡o hÃ m riÃªng): Gradient lÃ  vector cÃ¡c Ä‘áº¡o hÃ m riÃªng, Ã nghÄ©a trong tÃ¬m cá»±c tiá»ƒu hÃ m sá»‘
- ğŸ’» **CODE**: MÃ´ phá»ng thuáº­t toÃ¡n Gradient Descent tÃ¬m Ä‘iá»ƒm cá»±c tiá»ƒu cá»§a y = x^2 - 4x

#### NgÃ y 31

- ğŸ“˜ **LEARN**: Thá»‘ng kÃª mÃ´ táº£ (Descriptive Stats): Mean (Trung bÃ¬nh), Median (Trung vá»‹), Mode
- ğŸ“˜ **LEARN**: Expectation, Variance, Covariance/Correlation: cÃ¡ch Ä‘á»c tÆ°Æ¡ng quan, phÃ¢n biá»‡t correlation vs causation.
- ğŸ’» **CODE**: Viáº¿t hÃ m calculate_mean(data), calculate_median(data)

#### NgÃ y 32

- ğŸ“˜ **LEARN**: Variance & Standard Deviation (PhÆ°Æ¡ng sai & Äá»™ lá»‡ch chuáº©n): Äo Ä‘á»™ phÃ¢n tÃ¡n cá»§a dá»¯ liá»‡u, Táº¡i sao chia cho N hay N-1
- ğŸ“˜ **LEARN**: Sampling, Standard Error & Confidence Interval (CI): trá»±c giÃ¡c 'Ä‘á»™ cháº¯c cháº¯n' khi Æ°á»›c lÆ°á»£ng tá»« máº«u.
- ğŸ’» **CODE**: Viáº¿t hÃ m calculate_std(data) tá»« con sá»‘ 0

#### NgÃ y 33

- ğŸ“˜ **LEARN**: Probability Distributions (PhÃ¢n phá»‘i xÃ¡c suáº¥t): PhÃ¢n phá»‘i chuáº©n (Gaussian/Normal Distribution), HÃ m máº­t Ä‘á»™ xÃ¡c suáº¥t
- ğŸ“˜ **LEARN**: Conditional Probability & Bayes Theorem; cÃ¡c phÃ¢n phá»‘i hay gáº·p: Bernoulli/Binomial/Poisson/Exponential (Ã½ nghÄ©a + khi nÃ o dÃ¹ng).
- ğŸ’» **CODE**: DÃ¹ng random.gauss() Ä‘á»ƒ sinh dá»¯ liá»‡u giáº£ láº­p chiá»u cao con ngÆ°á»i

#### NgÃ y 34

- ğŸ“˜ **LEARN**: Softmax & Sigmoid Functions: HÃ m kÃ­ch hoáº¡t (Activation function), CÃ´ng thá»©c Softmax chuyá»ƒn Ä‘á»•i ra xÃ¡c suáº¥t
- ğŸ’» **CODE**: Viáº¿t hÃ m sigmoid(x) vÃ  softmax(list_x)

#### NgÃ y 35

- ğŸ“˜ **LEARN**: Project: Mini Numpy Part 2
- ğŸ“˜ **LEARN**: Hypothesis Testing & p-value (trá»±c giÃ¡c, khÃ´ng sa Ä‘Ã  cÃ´ng thá»©c); A/B testing: power, sample size, trÃ¡nh káº¿t luáº­n sai.
- ğŸ’» **CODE**: Project: MyStats Library. ThÃªm cÃ¡c hÃ m mean, std, softmax vÃ o thÆ° viá»‡n Mini Numpy. ÄÃ³ng gÃ³i thÃ nh module hoÃ n chá»‰nh.

### Tuáº§n 6: Data Structures & Algorithms (DSA) - Basic

_Viáº¿t code tá»‘i Æ°u hÆ¡n, chuáº©n bá»‹ cho phá»ng váº¥n._

#### NgÃ y 36

- ğŸ“˜ **LEARN**: Äá»™ phá»©c táº¡p thuáº­t toÃ¡n (Big O Notation): O(1), O(n), O(n^2), Táº¡i sao trÃ¡nh vÃ²ng láº·p lá»“ng nhau
- ğŸ’» **CODE**: So sÃ¡nh thá»i gian cháº¡y hÃ m tÃ¬m kiáº¿m O(n) vs O(1) (Dict lookup)

#### NgÃ y 37

- ğŸ“˜ **LEARN**: Array & String Manipulation (NÃ¢ng cao): Two Pointers technique, Sliding Window
- ğŸ’» **CODE**: LeetCode bÃ i Two Sum (phiÃªn báº£n tá»‘i Æ°u)

#### NgÃ y 38

- ğŸ“˜ **LEARN**: Hash Map / Dictionary (Deep Dive): Xá»­ lÃ½ Ä‘á»¥ng Ä‘á»™ (Collision), Khi nÃ o dÃ¹ng Hash Map
- ğŸ’» **CODE**: Giáº£i bÃ i toÃ¡n Ä‘áº¿m táº§n suáº¥t xuáº¥t hiá»‡n kÃ½ tá»± (dÃ¹ng Dict)

#### NgÃ y 39

- ğŸ“˜ **LEARN**: Stack & Queue: LIFO (Last In First Out), FIFO (First In First Out)
- ğŸ’» **CODE**: Implement Stack báº±ng List

#### NgÃ y 40

- ğŸ“˜ **LEARN**: Recursion (Äá»‡ quy): Äiá»u kiá»‡n dá»«ng, Stack overflow lÃ  gÃ¬
- ğŸ’» **CODE**: TÃ­nh giai thá»«a, dÃ£y Fibonacci báº±ng Ä‘á»‡ quy

#### NgÃ y 41

- ğŸ“˜ **LEARN**: Sorting Algorithms: Bubble Sort (Ä‘á»ƒ biáº¿t), Quick Sort / Merge Sort (Ä‘á»ƒ dÃ¹ng)
- ğŸ’» **CODE**: Implement Quick Sort Ä‘Æ¡n giáº£n

#### NgÃ y 42

- ğŸ“˜ **LEARN**: Binary Search (TÃ¬m kiáº¿m nhá»‹ phÃ¢n): TÃ¬m trong danh sÃ¡ch Ä‘Ã£ sáº¯p xáº¿p O(log n)
- ğŸ’» **CODE**: Viáº¿t hÃ m binary_search thá»§ cÃ´ng

### Tuáº§n 7: Tá»•ng há»£p & Capstone Project Stage 1

#### NgÃ y 43

- ğŸ“˜ **LEARN**: Unit Testing: Táº¡i sao cáº§n test, ThÆ° viá»‡n unittest/pytest cÆ¡ báº£n
- ğŸ’» **CODE**: Viáº¿t test case cho thÆ° viá»‡n Mini Numpy Ä‘Ã£ lÃ m

#### NgÃ y 44

- ğŸ“˜ **LEARN**: CAPSTONE PROJECT 1: Mini Numpy from Scratch (Final): Class Matrix: há»— trá»£ cá»™ng, nhÃ¢n, chuyá»ƒn vá»‹., Module Stats: tÃ­nh mean, std., Module Activation: softmax, sigmoid., Viáº¿t documentation (Readme.md) cÃ¡ch sá»­ dá»¥ng., Push lÃªn GitHub vá»›i cáº¥u trÃºc thÆ° má»¥c chuáº©n.

### Tuáº§n 8: Nghá»‰ ngÆ¡i & Review (Gap week)

_Ã”n táº­p láº¡i kiáº¿n thá»©c 7 tuáº§n qua, chuáº©n bá»‹ sang Stage 2 (Data Science)._

#### NgÃ y 45

- ğŸ“˜ **LEARN**: Review code cÅ©, Ä‘á»c sÃ¡ch 'Grokking Algorithms' hoáº·c nghá»‰ xáº£ hÆ¡i.

## Giai Ä‘oáº¡n 2: Data Analysis & Feature Engineering

**Thá»i gian:** Tuáº§n 9 Ä‘áº¿n Tuáº§n 16 (8 Tuáº§n)
**Má»¥c tiÃªu:** Biáº¿n dá»¯ liá»‡u thÃ´ thÃ nh thÃ´ng tin giÃ¡ trá»‹ (Insights) vÃ  Ä‘áº§u vÃ o cho Model.
**CÃ´ng cá»¥:** Pandas, NumPy, Matplotlib/Seaborn, SQL, Scikit-learn (Preprocessing)

### Tuáº§n 9: NumPy & Pandas Foundation (Cá»‘t lÃµi)

_Thao tÃ¡c thÃ nh tháº¡o DataFrame - Cáº¥u trÃºc dá»¯ liá»‡u quan trá»ng nháº¥t ngÃ nh Data._

#### NgÃ y 46

- ğŸ“˜ **LEARN**: NumPy to Real World: Broadcasting, Vectorization (Thay tháº¿ vÃ²ng for cháº­m cháº¡p), Indexing & Slicing nÃ¢ng cao
- ğŸ’» **CODE**: Tá»‘i Æ°u hÃ³a bÃ i toÃ¡n nhÃ¢n ma tráº­n Stage 1 báº±ng NumPy (1 dÃ²ng code).

#### NgÃ y 47

- ğŸ“˜ **LEARN**: Pandas Series & DataFrame: Cáº¥u trÃºc DataFrame, Äá»c dá»¯ liá»‡u (read_csv, read_excel, read_json)
- ğŸ’» **CODE**: Load file CSV 1 triá»‡u dÃ²ng, kiá»ƒm tra .info(), .describe().

#### NgÃ y 48

- ğŸ“˜ **LEARN**: Data Selection & Filtering: loc vs iloc, Boolean Indexing (Lá»c theo Ä‘iá»u kiá»‡n), Query method
- ğŸ’» **CODE**: Lá»c danh sÃ¡ch khÃ¡ch hÃ ng > 30 tuá»•i vÃ  mua hÃ ng > 100$.

#### NgÃ y 49

- ğŸ“˜ **LEARN**: Data Cleaning 1: Xá»­ lÃ½ dá»¯ liá»‡u lá»—i: Kiá»ƒm tra null (isna), XÃ³a null (dropna), Äiá»n giÃ¡ trá»‹ thiáº¿u (fillna)
- ğŸ’» **CODE**: Xá»­ lÃ½ dataset bá»‹ thiáº¿u tuá»•i: Ä‘iá»n báº±ng trung bÃ¬nh hoáº·c trung vá»‹.

#### NgÃ y 50

- ğŸ“˜ **LEARN**: Data Manipulation: Apply & Map: HÃ m .apply() (Powerful but slow), HÃ m .map(), Lambda functions trong Pandas
- ğŸ’» **CODE**: Táº¡o cá»™t má»›i 'AgeGroup' tá»« cá»™t 'Age' dÃ¹ng apply.

### Tuáº§n 10: Advanced Pandas & Aggregation

_Tráº£ lá»i cÃ¡c cÃ¢u há»i nghiá»‡p vá»¥ phá»©c táº¡p (Business Questions)._

#### NgÃ y 51

- ğŸ“˜ **LEARN**: GroupBy & Aggregation: Split-Apply-Combine strategy, CÃ¡c hÃ m agg (sum, mean, count)
- ğŸ’» **CODE**: TÃ­nh doanh thu trung bÃ¬nh theo tá»«ng thÃ nh phá»‘ vÃ  tá»«ng thÃ¡ng.

#### NgÃ y 52

- ğŸ“˜ **LEARN**: Merge & Join: Inner, Outer, Left, Right Join (Giá»‘ng SQL), Concat DataFrames
- ğŸ’» **CODE**: GhÃ©p báº£ng 'Users' vÃ  báº£ng 'Orders' Ä‘á»ƒ tÃ¬m ai mua nhiá»u nháº¥t.

#### NgÃ y 53

- ğŸ“˜ **LEARN**: Pivot Tables & Crosstab: Táº¡o báº£ng tá»•ng há»£p nhiá»u chiá»u (Giá»‘ng Excel Pivot), Stack/Unstack
- ğŸ’» **CODE**: Táº¡o Pivot Table so sÃ¡nh doanh sá»‘ theo NÄƒm (cá»™t) vÃ  Loáº¡i hÃ ng (dÃ²ng).

#### NgÃ y 54

- ğŸ“˜ **LEARN**: Time Series Data (Dá»¯ liá»‡u thá»i gian): datetime objects, Resampling (gom data theo ngÃ y/tuáº§n/thÃ¡ng), Time shifts
- ğŸ’» **CODE**: TÃ­nh doanh thu theo tá»«ng tuáº§n (Weekly Sales) tá»« data log tá»«ng giÃ¢y.

#### NgÃ y 55

- ğŸ“˜ **LEARN**: Mini Project Week 10
- ğŸ’» **CODE**: Project: Sales Analysis. Cho file sales.csv. Há»i: ThÃ¡ng nÃ o bÃ¡n tá»‘t nháº¥t? ThÃ nh phá»‘ nÃ o mua nhiá»u nháº¥t? Thá»i Ä‘iá»ƒm nÃ o trong ngÃ y khÃ¡ch hay mua?

### Tuáº§n 11: Data Visualization & Storytelling

_Váº½ hÃ¬nh Ä‘á»ƒ tÃ¬m Insight (khÃ´ng pháº£i váº½ cho Ä‘áº¹p)._

#### NgÃ y 56

- ğŸ“˜ **LEARN**: Matplotlib cÆ¡ báº£n: Figure, Axes structure, Line, Bar, Scatter plot
- ğŸ’» **CODE**: Váº½ biá»ƒu Ä‘á»“ Ä‘Æ°á»ng doanh thu theo thá»i gian.

#### NgÃ y 57

- ğŸ“˜ **LEARN**: Seaborn & Statistical Plots: Distplot (PhÃ¢n phá»‘i), Boxplot (PhÃ¡t hiá»‡n Outlier - Quan trá»ng), Violin plot
- ğŸ’» **CODE**: DÃ¹ng Boxplot tÃ¬m xem cÃ³ Ä‘Æ¡n hÃ ng nÃ o giÃ¡ trá»‹ cao báº¥t thÆ°á»ng khÃ´ng.

#### NgÃ y 58

- ğŸ“˜ **LEARN**: Multivariate Analysis (PhÃ¢n tÃ­ch Ä‘a biáº¿n): Heatmap (Ma tráº­n tÆ°Æ¡ng quan - Correlation Matrix), Pairplot
- ğŸ’» **CODE**: Váº½ Heatmap xem 'GiÃ¡ nhÃ ' tÆ°Æ¡ng quan tháº¿ nÃ o vá»›i 'Diá»‡n tÃ­ch' vÃ  'Sá»‘ phÃ²ng'.

#### NgÃ y 59

- ğŸ“˜ **LEARN**: Visual Storytelling: Chá»n Ä‘Ãºng loáº¡i biá»ƒu Ä‘á»“, MÃ u sáº¯c, TiÃªu Ä‘á», Label, TrÃ¡nh biá»ƒu Ä‘á»“ gÃ¢y hiá»ƒu láº§m
- ğŸ’» **CODE**: Refactor láº¡i cÃ¡c biá»ƒu Ä‘á»“ cÅ© cho chuyÃªn nghiá»‡p, sáºµn sÃ ng Ä‘á»ƒ bÃ¡o cÃ¡o.

### Tuáº§n 12: SQL for Data Science

_Láº¥y dá»¯ liá»‡u tá»« Database (Ká»¹ nÄƒng báº¯t buá»™c khi Ä‘i lÃ m)._

#### NgÃ y 60

- ğŸ“˜ **LEARN**: SQL Basic Review: SELECT, FROM, WHERE, ORDER BY, LIMIT
- ğŸ’» **CODE**: Viáº¿t query láº¥y Top 10 khÃ¡ch hÃ ng chi tiÃªu cao nháº¥t.

#### NgÃ y 61

- ğŸ“˜ **LEARN**: Joins & Unions: INNER vs LEFT JOIN, Xá»­ lÃ½ NULL sau khi Join
- ğŸ’» **CODE**: Join 3 báº£ng: Customers, Orders, Products.

#### NgÃ y 62

- ğŸ“˜ **LEARN**: Aggregation & Grouping: GROUP BY, HAVING (Lá»c sau khi Group)
- ğŸ’» **CODE**: TÃ¬m cÃ¡c danh má»¥c sáº£n pháº©m cÃ³ tá»•ng doanh thu > 10.000$.

#### NgÃ y 63

- ğŸ“˜ **LEARN**: Window Functions (NÃ¢ng cao - Cá»±c quan trá»ng): ROW_NUMBER(), RANK(), LEAD/LAG (So sÃ¡nh dÃ²ng trÆ°á»›c/sau)
- ğŸ’» **CODE**: TÃ¬m top 3 nhÃ¢n viÃªn xuáº¥t sáº¯c nháº¥t cá»§a Tá»ªNG phÃ²ng ban (Partition By).

#### NgÃ y 64

- ğŸ“˜ **LEARN**: CTEs (Common Table Expressions): WITH clause, Viáº¿t query dá»… Ä‘á»c hÆ¡n
- ğŸ’» **CODE**: Viáº¿t CTE tÃ­nh doanh thu thÃ¡ng, sau Ä‘Ã³ query chÃ­nh tÃ­nh tÄƒng trÆ°á»Ÿng so vá»›i thÃ¡ng trÆ°á»›c.

### Tuáº§n 13: Feature Engineering (Ká»¹ thuáº­t Ä‘áº·c trÆ°ng)

_Biáº¿n dá»¯ liá»‡u thÃ´ thÃ nh dá»¯ liá»‡u mÃ  Model hiá»ƒu Ä‘Æ°á»£c._

#### NgÃ y 65

- ğŸ“˜ **LEARN**: Feature Scaling (Chuáº©n hÃ³a): Normalization (Min-Max), Standardization (Z-score), Khi nÃ o dÃ¹ng cÃ¡i nÃ o?
- ğŸ’» **CODE**: DÃ¹ng Scikit-learn StandardScaler Ä‘á»ƒ chuáº©n hÃ³a cá»™t 'Salary'.

#### NgÃ y 66

- ğŸ“˜ **LEARN**: Encoding Categorical Data (MÃ£ hÃ³a phÃ¢n loáº¡i): One-Hot Encoding, Label Encoding, Target Encoding
- ğŸ’» **CODE**: Chuyá»ƒn cá»™t 'MÃ u sáº¯c' (Äá», Xanh) thÃ nh vector sá»‘ há»c.

#### NgÃ y 67

- ğŸ“˜ **LEARN**: Handling Outliers (Xá»­ lÃ½ ngoáº¡i lai): IQR Method, Z-score method, Capping/Flooring
- ğŸ’» **CODE**: Viáº¿t hÃ m tá»± Ä‘á»™ng loáº¡i bá» cÃ¡c giÃ¡ trá»‹ ngoáº¡i lai trong Dataset.

#### NgÃ y 68

- ğŸ“˜ **LEARN**: Feature Selection: Correlation filter, Variance threshold, Táº¡i sao Ã­t feature láº¡i tá»‘t hÆ¡n?
- ğŸ’» **CODE**: Loáº¡i bá» cÃ¡c cá»™t cÃ³ Ä‘á»™ tÆ°Æ¡ng quan > 0.9 (DÆ° thá»«a).

### Tuáº§n 14: Polars & Modern Tools (Optional but Recommended)

_LÃ m quen cÃ´ng cá»¥ xá»­ lÃ½ dá»¯ liá»‡u lá»›n tá»‘c Ä‘á»™ cao._

#### NgÃ y 69

- ğŸ“˜ **LEARN**: Intro to Polars: Lazy Evaluation, So sÃ¡nh tá»‘c Ä‘á»™ vá»›i Pandas
- ğŸ’» **CODE**: Viáº¿t láº¡i bÃ i toÃ¡n GroupBy cá»§a Pandas báº±ng Polars.

### Tuáº§n 15 (Bá»• sung): Big Data vá»›i Apache Spark & PySpark

_LÃ m quen vá»›i xá»­ lÃ½ dá»¯ liá»‡u lá»›n phÃ¢n tÃ¡n báº±ng Apache Spark/PySpark, chuáº©n bá»‹ cho cÃ¡c há»‡ thá»‘ng AI quy mÃ´ doanh nghiá»‡p._

#### NgÃ y 70

- ğŸ“˜ **LEARN**: Giá»›i thiá»‡u Big Data & Apache Spark: Cluster, Driver, Executor, RDD vs DataFrame. Táº¡i sao Spark nhanh hÆ¡n chá»‰ dÃ¹ng Pandas?
- ğŸ’» **CODE**: CÃ i Ä‘áº·t PySpark (local). Viáº¿t script Ä‘Æ¡n giáº£n Ä‘á»c file CSV lá»›n, gá»i .count() vÃ  .show().

#### NgÃ y 71

- ğŸ“˜ **LEARN**: Transformations & Actions trong Spark: select, filter, withColumn, groupBy, agg. Lazy evaluation lÃ  gÃ¬?
- ğŸ’» **CODE**: DÃ¹ng PySpark DataFrame Ä‘á»ƒ tÃ­nh tá»•ng doanh thu theo ngÃ y/thÃ¡ng trÃªn dataset > 1GB (giáº£ láº­p náº¿u cáº§n).

#### NgÃ y 72

- ğŸ“˜ **LEARN**: Joins trong Spark: inner, left, right, full outer. Khi nÃ o cáº§n repartition/broadcast join Ä‘á»ƒ tá»‘i Æ°u?
- ğŸ’» **CODE**: GhÃ©p 2 báº£ng lá»›n (Users, Events) báº±ng PySpark, tÃ­nh DAU/MAU, top 10 user cÃ³ nhiá»u event nháº¥t.

#### NgÃ y 73

- ğŸ“˜ **LEARN**: Partitioning & Caching: partitionBy, coalesce, persist/cache. Äá»c hiá»ƒu Spark UI vÃ  explain() Ä‘á»ƒ tá»‘i Æ°u query.
- ğŸ’» **CODE**: So sÃ¡nh thá»i gian cháº¡y: cÃ¹ng má»™t pipeline ETL nhÆ°ng cÃ³/khÃ´ng cÃ³ cache(), cÃ³/khÃ´ng cÃ³ partition há»£p lÃ½.

#### NgÃ y 74

- ğŸ“˜ **LEARN**: Spark trÃªn Cloud (high-level): Databricks, EMR, GCP Dataproc. KhÃ¡i niá»‡m job, cluster size, autoscaling.
- ğŸ’» **CODE**: ÄÃ³ng gÃ³i notebook/py script ETL báº±ng PySpark: Ä‘á»c dá»¯ liá»‡u raw â†’ lÃ m sáº¡ch â†’ aggregate â†’ ghi ra Parquet/Delta. Viáº¿t README mÃ´ táº£ pipeline.

### Tuáº§n 16 (Bá»• sung): Data Modeling, Data Quality & Lakehouse Basics

_Náº¯m tÆ° duy há»‡ dá»¯ liá»‡u Ä‘á»ƒ xÃ¢y pipeline ML/analytics bá»n vá»¯ng: modeling, Ä‘á»‹nh dáº¡ng lÆ°u trá»¯, cháº¥t lÆ°á»£ng dá»¯ liá»‡u, versioning/lineage._

#### NgÃ y 75

- ğŸ“˜ **LEARN**: Data Modeling 101: OLTP vs OLAP, Fact/Dimension, grain; Star schema (tÆ° duy thiáº¿t káº¿ báº£ng).
- ğŸ’» **CODE**: Chá»n 1 dataset (e-commerce) vÃ  phÃ¡c tháº£o star schema: dim_date, dim_user, dim_product, fact_orders (viáº¿t markdown + táº¡o dataframe máº«u).

#### NgÃ y 76

- ğŸ“˜ **LEARN**: File Formats & Storage: CSV vs Parquet; row vs columnar, compression, predicate pushdown; partitioning theo time/key.
- ğŸ’» **CODE**: LÆ°u dá»¯ liá»‡u sang Parquet vÃ  so sÃ¡nh kÃ­ch thÆ°á»›c/ tá»‘c Ä‘á»™ Ä‘á»c; thá»­ partition theo date (local).

#### NgÃ y 77

- ğŸ“˜ **LEARN**: Lakehouse basics: ACID, schema evolution, time travel; table formats (Delta Lake / Iceberg / Hudi) â€“ khÃ¡i niá»‡m & trade-off.
- ğŸ’» **CODE**: Viáº¿t note 1 trang: khi nÃ o dÃ¹ng warehouse vs lake vs lakehouse; náº¿u cÃ³ mÃ´i trÆ°á»ng: demo ghi 2 phiÃªn báº£n Parquet + lÆ°u checksum/version.

#### NgÃ y 78

- ğŸ“˜ **LEARN**: Data Quality & Validation: profiling, schema checks, range/uniqueness, null/duplicate rules; Great Expectations / Pandera (khÃ¡i niá»‡m).
- ğŸ’» **CODE**: Viáº¿t 5â€“10 rule kiá»ƒm tra dá»¯ liá»‡u báº±ng Pandera (hoáº·c Great Expectations) vÃ  xuáº¥t report lá»—i.

#### NgÃ y 79

- ğŸ“˜ **LEARN**: Data Versioning & Lineage: vÃ¬ sao ML cáº§n reproducibility; data lineage, metadata; giá»›i thiá»‡u DVC/lakeFS (khÃ¡i niá»‡m).
- ğŸ’» **CODE**: Táº¡o pipeline mini: validate -> save cleaned dataset -> log metadata (hash, rows, columns) vÃ o file JSON/YAML; fail náº¿u vi pháº¡m rules.

### CAPSTONE PROJECT GIAI ÄOáº N 2

_Tá»•ng há»£p toÃ n bá»™ kiáº¿n thá»©c Data Analysis & Feature Engineering Ä‘á»ƒ lÃ m 1 mini-project end-to-end._

#### NgÃ y 80

- ğŸ“˜ **LEARN**: Project: End-to-End Exploratory Data Analysis (EDA)
- ğŸ“˜ **LEARN**: Dataset: Shopee/Tiki E-commerce Dataset hoáº·c Kaggle Titanic (Advanced version)
- ğŸ’» **CODE**: 1. Data Cleaning: Xá»­ lÃ½ null, duplicate, sai format.
- ğŸ’» **CODE**: 2. Feature Engineering: Táº¡o cá»™t má»›i (VD: TÃ¡ch 'Title' tá»« 'Name', nhÃ³m tuá»•i).
- ğŸ’» **CODE**: 3. Visualization: 5-7 biá»ƒu Ä‘á»“ tráº£ lá»i cÃ¡c cÃ¢u há»i Insight.
- ğŸ’» **CODE**: 4. Correlation Analysis: TÃ¬m cÃ¡c yáº¿u tá»‘ áº£nh hÆ°á»Ÿng Ä‘áº¿n má»¥c tiÃªu (VD: GiÃ¡ vÃ© áº£nh hÆ°á»Ÿng tháº¿ nÃ o Ä‘áº¿n tá»· lá»‡ sá»‘ng sÃ³t).
- ğŸ’» **CODE**: 5. Report: Xuáº¥t file Jupyter Notebook (.ipynb) sáº¡ch Ä‘áº¹p, cÃ³ chÃº thÃ­ch Markdown giáº£i thÃ­ch tá»«ng bÆ°á»›c.

## Giai Ä‘oáº¡n 3: Machine Learning Thá»±c chiáº¿n (Classical ML)

**Thá»i gian:** Tuáº§n 17 Ä‘áº¿n Tuáº§n 26 (10 Tuáº§n)
**Má»¥c tiÃªu:** ThÃ nh tháº¡o cÃ¡c thuáº­t toÃ¡n ML cá»‘t lÃµi cho dá»¯ liá»‡u báº£ng (Tabular Data).
**CÃ´ng cá»¥:** Scikit-learn, XGBoost, LightGBM, Optuna, SHAP, Imbalanced-learn

### Tuáº§n 17: ML Overview & Linear Regression

_Hiá»ƒu luá»“ng Ä‘i cá»§a má»™t bÃ i toÃ¡n ML: Train, Test, Predict._

#### NgÃ y 81

- ğŸ“˜ **LEARN**: Machine Learning Workflow: Supervised vs Unsupervised, Train/Test Split (Táº¡i sao pháº£i chia data?)
- ğŸ“˜ **LEARN**: Data Leakage & Split Ä‘Ãºng cÃ¡ch: stratified split cho classification; fit scaler/encoder chá»‰ trÃªn train; pipeline Ä‘á»ƒ trÃ¡nh leakage.
- ğŸ’» **CODE**: DÃ¹ng sklearn.model_selection.train_test_split chia dá»¯ liá»‡u.

#### NgÃ y 82

- ğŸ“˜ **LEARN**: Linear Regression (Há»“i quy tuyáº¿n tÃ­nh) - LÃ½ thuyáº¿t: PhÆ°Æ¡ng trÃ¬nh Ä‘Æ°á»ng tháº³ng y = ax + b, Loss Function (MSE), Gradient Descent (Ã´n láº¡i)
- ğŸ’» **CODE**: Implement Linear Regression báº±ng Numpy (Ã´n láº¡i) vs dÃ¹ng Sklearn.

#### NgÃ y 83

- ğŸ“˜ **LEARN**: Linear Regression - Thá»±c hÃ nh: Há»‡ sá»‘ (Coefficients), Intercept, Äa cá»™ng tuyáº¿n (Multicollinearity)
- ğŸ’» **CODE**: Dá»± Ä‘oÃ¡n giÃ¡ nhÃ  (Boston Housing/California Housing dataset).

#### NgÃ y 84

- ğŸ“˜ **LEARN**: Polynomial Regression: Khi Ä‘Æ°á»ng tháº³ng khÃ´ng Ä‘á»§ tá»‘t (Underfitting), Táº¡o feature báº­c cao
- ğŸ’» **CODE**: DÃ¹ng PolynomialFeatures cá»§a sklearn Ä‘á»ƒ fit Ä‘Æ°á»ng cong.

### Tuáº§n 18: Classification (PhÃ¢n loáº¡i) & Logistic Regression

_Giáº£i quyáº¿t bÃ i toÃ¡n Yes/No (Spam hay khÃ´ng Spam)._

#### NgÃ y 85

- ğŸ“˜ **LEARN**: Logistic Regression: Sigmoid Function, Decision Boundary, Táº¡i sao dÃ¹ng Log Loss?
- ğŸ’» **CODE**: PhÃ¢n loáº¡i hoa Iris (Binary Classification).

#### NgÃ y 86

- ğŸ“˜ **LEARN**: Evaluation Metrics cho PhÃ¢n loáº¡i (Cá»°C QUAN TRá»ŒNG): Accuracy (dá»… lá»«a), Precision & Recall (Quan trá»ng), F1-Score, Confusion Matrix
- ğŸ“˜ **LEARN**: Calibration & Thresholding: xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cÃ³ 'Ä‘Ãºng xÃ¡c suáº¥t' khÃ´ng? (Platt/Isotonic); chá»n threshold theo cost/precision-recall.
- ğŸ’» **CODE**: TÃ­nh tay Precision/Recall tá»« Confusion Matrix.

#### NgÃ y 87

- ğŸ“˜ **LEARN**: ROC Curve & AUC: True Positive Rate vs False Positive Rate, Threshold tuning (Chá»‰nh ngÆ°á»¡ng quyáº¿t Ä‘á»‹nh)
- ğŸ’» **CODE**: Váº½ Ä‘Æ°á»ng ROC vÃ  tÃ­nh diá»‡n tÃ­ch AUC.

### Tuáº§n 19: KNN & SVM (Support Vector Machines)

_Hiá»ƒu cÃ¡c thuáº­t toÃ¡n dá»±a trÃªn khoáº£ng cÃ¡ch._

#### NgÃ y 88

- ğŸ“˜ **LEARN**: K-Nearest Neighbors (KNN): Euclidean distance, Chá»n K tháº¿ nÃ o?, Lá»i nguyá»n cá»§a sá»‘ chiá»u (Curse of Dimensionality)
- ğŸ’» **CODE**: KNN classifier cho dataset chá»¯ viáº¿t tay (MNIST nhá»).

#### NgÃ y 89

- ğŸ“˜ **LEARN**: Support Vector Machines (SVM): Hyperplane, Margin tá»‘i Ä‘a, Kernel Trick (biáº¿n khÃ´ng gian cong thÃ nh pháº³ng)
- ğŸ’» **CODE**: So sÃ¡nh Linear SVM vs RBF Kernel SVM.

### Tuáº§n 20: Decision Trees & Bias-Variance Tradeoff

_Ná»n táº£ng cá»§a cÃ¡c thuáº­t toÃ¡n máº¡nh nháº¥t hiá»‡n nay._

#### NgÃ y 90

- ğŸ“˜ **LEARN**: Decision Trees (CÃ¢y quyáº¿t Ä‘á»‹nh): Entropy & Gini Impurity (CÃ¡ch cÃ¢y Ä‘áº·t cÃ¢u há»i), Pruning (Cáº¯t tá»‰a cÃ¢y Ä‘á»ƒ chá»‘ng Overfitting)
- ğŸ’» **CODE**: Visual hÃ³a cÃ¢y quyáº¿t Ä‘á»‹nh báº±ng graphviz.

#### NgÃ y 91

- ğŸ“˜ **LEARN**: Bias vs Variance (Äá»™ lá»‡ch vs PhÆ°Æ¡ng sai): Overfitting (Há»c váº¹t) vs Underfitting (Há»c kÃ©m), CÃ¡ch phÃ¡t hiá»‡n qua Learning Curve
- ğŸ’» **CODE**: Váº½ Learning Curve Ä‘á»ƒ cháº©n Ä‘oÃ¡n model.

#### NgÃ y 92

- ğŸ“˜ **LEARN**: Cross-Validation (Kiá»ƒm Ä‘á»‹nh chÃ©o): K-Fold CV, Stratified K-Fold (Giá»¯ tá»· lá»‡ nhÃ£n)
- ğŸ“˜ **LEARN**: Split theo ngá»¯ cáº£nh: TimeSeriesSplit, GroupKFold (trÃ¡nh rÃ² rá»‰ theo user/session); Nested CV khi tuning hyperparameters.
- ğŸ’» **CODE**: Ãp dá»¥ng cross_val_score thay vÃ¬ chá»‰ train/test split.

### Tuáº§n 21: Ensemble Learning & Random Forest

_Sá»©c máº¡nh cá»§a Ä‘Ã¡m Ä‘Ã´ng (Nhiá»u cÃ¢y yáº¿u gá»™p láº¡i thÃ nh rá»«ng máº¡nh)._

#### NgÃ y 93

- ğŸ“˜ **LEARN**: Bagging & Random Forest: Bootstrap Aggregating, Táº¡i sao Random Forest khÃ³ bá»‹ Overfit?, Feature Importance
- ğŸ’» **CODE**: Train Random Forest Classifier. Xem feature nÃ o quan trá»ng nháº¥t.

#### NgÃ y 94

- ğŸ“˜ **LEARN**: Ensemble Voting: Hard Voting vs Soft Voting
- ğŸ’» **CODE**: Káº¿t há»£p káº¿t quáº£ cá»§a Logistic Regression, KNN vÃ  Random Forest.

### Tuáº§n 22: Boosting Algorithms (VÅ© khÃ­ tá»‘i thÆ°á»£ng cho Tabular Data)

_ThÃ nh tháº¡o XGBoost/LightGBM - Thá»© doanh nghiá»‡p dÃ¹ng nhiá»u nháº¥t._

#### NgÃ y 95

- ğŸ“˜ **LEARN**: Boosting Concept: Há»c tuáº§n tá»± (Sequential Learning), Sá»­a sai cho model trÆ°á»›c
- ğŸ’» **CODE**: AdaBoost cÆ¡ báº£n.

#### NgÃ y 96

- ğŸ“˜ **LEARN**: XGBoost (Extreme Gradient Boosting): Táº¡i sao XGBoost vÃ´ Ä‘á»‹ch Kaggle?, Regularization tÃ­ch há»£p
- ğŸ’» **CODE**: CÃ i Ä‘áº·t XGBoost, train model vÃ  sá»­ dá»¥ng early_stopping.

#### NgÃ y 97

- ğŸ“˜ **LEARN**: LightGBM & CatBoost: Tá»‘c Ä‘á»™ cá»§a LightGBM (Histogram-based), Xá»­ lÃ½ category cá»§a CatBoost
- ğŸ’» **CODE**: So sÃ¡nh tá»‘c Ä‘á»™ train giá»¯a Random Forest vs XGBoost vs LightGBM.

### Tuáº§n 23: Unsupervised Learning

_TÃ¬m áº©n sá»‘ khi khÃ´ng cÃ³ nhÃ£n (Label)._

#### NgÃ y 98

- ğŸ“˜ **LEARN**: Clustering: K-Means: Elbow Method (Chá»n K tá»‘i Æ°u), K-Means++ init
- ğŸ’» **CODE**: PhÃ¢n nhÃ³m khÃ¡ch hÃ ng dá»±a trÃªn hÃ nh vi mua sáº¯m.

#### NgÃ y 99

- ğŸ“˜ **LEARN**: Dimensionality Reduction: PCA: Giáº£m chiá»u dá»¯ liá»‡u, Giá»¯ láº¡i bao nhiÃªu thÃ´ng tin (Variance explained)?
- ğŸ’» **CODE**: Giáº£m dataset tá»« 100 chiá»u xuá»‘ng 2 chiá»u Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“.

#### NgÃ y 100

- ğŸ“˜ **LEARN**: Clustering nÃ¢ng cao: DBSCAN: PhÃ¡t hiá»‡n cá»¥m hÃ¬nh dáº¡ng báº¥t ká»³, Xá»­ lÃ½ nhiá»…u (Noise/Outliers)
- ğŸ’» **CODE**: So sÃ¡nh káº¿t quáº£ K-Means vs DBSCAN trÃªn dá»¯ liá»‡u hÃ¬nh trÄƒng khuyáº¿t.

### Tuáº§n 24: Advanced ML Techniques

_Xá»­ lÃ½ cÃ¡c váº¥n Ä‘á» thá»±c táº¿ khÃ³ nháº±n._

#### NgÃ y 101

- ğŸ“˜ **LEARN**: Imbalanced Data (Dá»¯ liá»‡u máº¥t cÃ¢n báº±ng): Táº¡i sao Accuracy vÃ´ dá»¥ng á»Ÿ Ä‘Ã¢y?, Undersampling vs Oversampling (SMOTE)
- ğŸ’» **CODE**: DÃ¹ng thÆ° viá»‡n imbalanced-learn Ä‘á»ƒ sinh dá»¯ liá»‡u giáº£ láº­p class hiáº¿m.

#### NgÃ y 102

- ğŸ“˜ **LEARN**: Hyperparameter Tuning: Grid Search vs Random Search, Bayesian Optimization (Optuna)
- ğŸ’» **CODE**: DÃ¹ng Optuna Ä‘á»ƒ tá»± Ä‘á»™ng tÃ¬m tham sá»‘ tá»‘t nháº¥t cho XGBoost.

#### NgÃ y 103

- ğŸ“˜ **LEARN**: Explainable AI (SHAP): Black box model, SHAP values: Táº¡i sao model dá»± Ä‘oÃ¡n ngÆ°á»i nÃ y rá»§i ro cao?
- ğŸ’» **CODE**: Váº½ biá»ƒu Ä‘á»“ SHAP summary plot.

### Tuáº§n 25-26: CAPSTONE PROJECT GIAI ÄOáº N 3

#### NgÃ y 104

- ğŸ“˜ **LEARN**: Project: Credit Scoring / Customer Churn Prediction
- ğŸ“˜ **LEARN**: Dataset: Telco Customer Churn (Kaggle) hoáº·c German Credit Data.
- ğŸ’» **CODE**: 1. Pipeline: XÃ¢y dá»±ng Sklearn Pipeline hoÃ n chá»‰nh (FillNA -> Scale -> Model).
- ğŸ’» **CODE**: 2. Handling Imbalance: Ãp dá»¥ng SMOTE vÃ¬ sá»‘ lÆ°á»£ng khÃ¡ch rá»i bá» thÆ°á»ng Ã­t.
- ğŸ’» **CODE**: 3. Model Selection: Thá»­ nghiá»‡m Logistic Regression, Random Forest vÃ  XGBoost.
- ğŸ’» **CODE**: 4. Tuning: DÃ¹ng Optuna tá»‘i Æ°u F1-Score (hoáº·c ROC-AUC).
- ğŸ’» **CODE**: 5. Explain: DÃ¹ng SHAP Ä‘á»ƒ giáº£i thÃ­ch cho sáº¿p biáº¿t: 'Yáº¿u tá»‘ nÃ o khiáº¿n khÃ¡ch hÃ ng bá» Ä‘i nhiá»u nháº¥t?' (GiÃ¡ cÆ°á»›c? Dá»‹ch vá»¥ CSKH?).
- ğŸ’» **CODE**: 6. Save Model: LÆ°u model dáº¡ng .pkl hoáº·c .json Ä‘á»ƒ dÃ¹ng sau nÃ y.

## Giai Ä‘oáº¡n 4: Deep Learning Foundation (Updated)

**Thá»i gian:** Tuáº§n 27 Ä‘áº¿n Tuáº§n 42 (16 Tuáº§n)
**Má»¥c tiÃªu:** XÃ¢y dá»±ng tÆ° duy Deep Learning, thÃ nh tháº¡o PyTorch vÃ  náº¯m vá»¯ng kiáº¿n trÃºc CNN/RNN/Transformer.
**CÃ´ng cá»¥:** PyTorch, Torchvision, OpenCV, TensorBoard / Weights & Biases, Google Colab (GPU Free)

### Tuáº§n 27: Neural Networks (NN) - Tá»« con sá»‘ 0

_Hiá»ƒu cáº¥u táº¡o cá»§a má»™t 'Brain cell' nhÃ¢n táº¡o._

#### NgÃ y 105

- ğŸ“˜ **LEARN**: Perceptron & Neuron: Má»‘i liÃªn há»‡ giá»¯a Logistic Regression vÃ  1 Neuron, Weights (Trá»ng sá»‘) & Bias
- ğŸ’» **CODE**: Code láº¡i 1 Neuron báº±ng Python thuáº§n.

#### NgÃ y 106

- ğŸ“˜ **LEARN**: Activation Functions (HÃ m kÃ­ch hoáº¡t): Táº¡i sao cáº§n phi tuyáº¿n tÃ­nh?, Sigmoid vs Tanh vs ReLU (Rectified Linear Unit)
- ğŸ’» **CODE**: Váº½ Ä‘á»“ thá»‹ cÃ¡c hÃ m activation báº±ng Matplotlib.

#### NgÃ y 107

- ğŸ“˜ **LEARN**: Multi-Layer Perceptron (MLP): Input Layer, Hidden Layers, Output Layer, Feed Forward (Lan truyá»n xuÃ´i)
- ğŸ“˜ **LEARN**: Computational Graph (trá»±c giÃ¡c) & shapes: forward pass táº¡o graph, backprop lÃ  lan truyá»n gradient theo graph.
- ğŸ’» **CODE**: XÃ¢y dá»±ng máº¡ng MLP Ä‘Æ¡n giáº£n phÃ¢n loáº¡i dá»¯ liá»‡u XOR.

### Tuáº§n 28: Backpropagation (TrÃ¡i tim cá»§a DL)

_Hiá»ƒu cÃ¡ch model tá»± sá»­a sai (Há»c)._

#### NgÃ y 108

- ğŸ“˜ **LEARN**: Loss Functions: MSE (cho Regression), Cross-Entropy Loss (cho Classification)
- ğŸ“˜ **LEARN**: Jacobian/Hessian (trá»±c giÃ¡c): Jacobian lÃ  'Ä‘áº¡o hÃ m cá»§a vector'; Hessian nÃ³i vá» Ä‘á»™ cong â€“ giÃºp hiá»ƒu tá»‘i Æ°u vÃ  learning rate.
- ğŸ’» **CODE**: TÃ­nh tay Loss cá»§a má»™t dá»± Ä‘oÃ¡n sai.

#### NgÃ y 109

- ğŸ“˜ **LEARN**: Backpropagation Theory: Chain Rule (Quy táº¯c chuá»—i - Ã”n láº¡i Calculus), Äáº¡o hÃ m cá»§a Loss theo Weight
- ğŸ’» **CODE**: Xem video 3Blue1Brown vá» Backpropagation (Báº¯t buá»™c).

#### NgÃ y 110

- ğŸ“˜ **LEARN**: Optimization Algorithms: SGD (Stochastic Gradient Descent), Adam (Adaptive Moment Estimation - DÃ¹ng máº·c Ä‘á»‹nh)
- ğŸ“˜ **LEARN**: Thá»±c chiáº¿n Optimization: Momentum/Nesterov, Weight Decay vs L2, Learning Rate Schedules, Gradient Clipping; khi nÃ o dÃ¹ng vÃ  dáº¥u hiá»‡u cáº§n.
- ğŸ’» **CODE**: So sÃ¡nh tá»‘c Ä‘á»™ há»™i tá»¥ cá»§a SGD vs Adam.

### Tuáº§n 29-30: PyTorch Framework Mastery

_Code Deep Learning chuyÃªn nghiá»‡p & Quáº£n lÃ½ thÃ­ nghiá»‡m._

#### NgÃ y 111

- ğŸ“˜ **LEARN**: Tensors & Autograd: Tensor lÃ  gÃ¬? (KhÃ¡c gÃ¬ NumPy array?), GPU acceleration (cuda), Autograd (Tá»± Ä‘á»™ng tÃ­nh Ä‘áº¡o hÃ m)
- ğŸ’» **CODE**: Chuyá»ƒn Ä‘á»•i qua láº¡i giá»¯a Numpy vÃ  Tensor. TÃ­nh Ä‘áº¡o hÃ m tá»± Ä‘á»™ng y.backward().

#### NgÃ y 112

- ğŸ“˜ **LEARN**: PyTorch Workflow: Dataset & DataLoader: Class Dataset (len, getitem), DataLoader (Batching, Shuffling)
- ğŸ’» **CODE**: Viáº¿t custom Dataset Ä‘á»ƒ load áº£nh tá»« folder.

#### NgÃ y 113

- ğŸ“˜ **LEARN**: Building Model: nn.Module: HÃ m **init** vÃ  forward(), nn.Linear, nn.Sequential
- ğŸ’» **CODE**: Viáº¿t class ImageClassifier káº¿ thá»«a nn.Module.

#### NgÃ y 114

- ğŸ“˜ **LEARN**: Training Loop & Visualization: 5 bÆ°á»›c chuáº©n: Forward -> Loss -> Zero_grad -> Backward -> Step, Sá»­ dá»¥ng TensorBoard Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“ Loss realtime
- ğŸ’» **CODE**: TÃ­ch há»£p TensorBoardWriter vÃ o vÃ²ng láº·p train.

#### NgÃ y 115

- ğŸ“˜ **LEARN**: Save/Load & Project MNIST: torch.save (state_dict), torch.load (Resume training)
- ğŸ’» **CODE**: Project: Handwritten Digit Recognition (Full pipeline: Train -> Save -> Load -> Predict).

### Tuáº§n 31-33: Computer Vision (CNN)

_Xá»­ lÃ½ dá»¯ liá»‡u hÃ¬nh áº£nh & Autoencoders._

#### NgÃ y 116

- ğŸ“˜ **LEARN**: Convolution Operation (TÃ­ch cháº­p): Kernel/Filter, Stride, Padding, Táº¡i sao CNN tá»‘t hÆ¡n MLP cho áº£nh?
- ğŸ’» **CODE**: TÃ­nh tay káº¿t quáº£ tÃ­ch cháº­p cá»§a ma tráº­n 5x5 vá»›i filter 3x3.

#### NgÃ y 117

- ğŸ“˜ **LEARN**: Pooling Layers & Architecture: Max Pooling vs Average Pooling, Cáº¥u trÃºc kinh Ä‘iá»ƒn: Conv -> Relu -> Pool
- ğŸ’» **CODE**: XÃ¢y dá»±ng mÃ´ hÃ¬nh LeNet-5 cá»• Ä‘iá»ƒn.

#### NgÃ y 118

- ğŸ“˜ **LEARN**: Modern Architectures: ResNet: Vanishing Gradient Problem, Skip Connections (Residual Block), Táº¡i sao ResNet sÃ¢u Ä‘Æ°á»£c?
- ğŸ’» **CODE**: DÃ¹ng torchvision.models.resnet18 (pretrained=True).

#### NgÃ y 119

- ğŸ“˜ **LEARN**: Autoencoders (Intro to Generative): Encoder - Decoder Architecture, Latent Space (KhÃ´ng gian áº©n), á»¨ng dá»¥ng: Khá»­ nhiá»…u áº£nh (Denoising)
- ğŸ’» **CODE**: XÃ¢y dá»±ng Autoencoder Ä‘Æ¡n giáº£n Ä‘á»ƒ nÃ©n vÃ  giáº£i nÃ©n áº£nh MNIST.

#### NgÃ y 120

- ğŸ“˜ **LEARN**: Transfer Learning & Object Detection Intro: Fine-tuning vs Feature Extraction, KhÃ¡i niá»‡m YOLO (You Only Look Once)
- ğŸ’» **CODE**: DÃ¹ng ResNet Ä‘Ã£ há»c ImageNet Ä‘á»ƒ phÃ¢n loáº¡i ChÃ³/MÃ¨o.

### Tuáº§n 34-36: Sequence Models (RNN & NLP Foundation)

_Xá»­ lÃ½ dá»¯ liá»‡u chuá»—i (Text, Time Series)._

#### NgÃ y 121

- ğŸ“˜ **LEARN**: Text Preprocessing: Tokenization, Stopwords, Stemming/Lemmatization, One-hot encoding text
- ğŸ’» **CODE**: DÃ¹ng thÆ° viá»‡n NLTK hoáº·c spaCy Ä‘á»ƒ xá»­ lÃ½ cÃ¢u vÄƒn.

#### NgÃ y 122

- ğŸ“˜ **LEARN**: Word Embeddings: Táº¡i sao One-hot tá»‡?, Word2Vec idea, Embedding Layer trong PyTorch
- ğŸ’» **CODE**: Visual hÃ³a vector tá»« vá»±ng (King - Man + Woman = Queen).

#### NgÃ y 123

- ğŸ“˜ **LEARN**: RNN (Recurrent Neural Networks): Hidden State (TrÃ­ nhá»› ngáº¯n háº¡n), Váº¥n Ä‘á» Vanishing Gradient trong RNN
- ğŸ’» **CODE**: Viáº¿t RNN Ä‘Æ¡n giáº£n dá»± Ä‘oÃ¡n kÃ½ tá»± tiáº¿p theo.

#### NgÃ y 124

- ğŸ“˜ **LEARN**: LSTM & GRU: Gates (Cá»•ng quÃªn, cá»•ng nháº­p), Long Short-Term Memory
- ğŸ’» **CODE**: PhÃ¢n loáº¡i cáº£m xÃºc bÃ¬nh luáº­n phim (IMDB) dÃ¹ng LSTM.

### Tuáº§n 37-38: Attention & Transformers (Cáº§u ná»‘i Ä‘áº¿n LLM)

_Hiá»ƒu kiáº¿n trÃºc Ä‘Ã£ thay Ä‘á»•i tháº¿ giá»›i AI._

#### NgÃ y 125

- ğŸ“˜ **LEARN**: Seq2Seq & Attention Mechanism: Encoder-Decoder Architecture, Bahdanau Attention (Táº¡i sao pháº£i focus vÃ o tá»«ng pháº§n?)
- ğŸ’» **CODE**: Minh há»a cÆ¡ cháº¿ Attention báº±ng heatmap.

#### NgÃ y 126

- ğŸ“˜ **LEARN**: Transformer Architecture (Paper: Attention is All You Need): Self-Attention, Multi-Head Attention, Positional Encoding
- ğŸ’» **CODE**: Äá»c vÃ  cháº¡y thá»­ code Transformer PyTorch tutorial.

#### NgÃ y 127

- ğŸ“˜ **LEARN**: BERT vs GPT: Encoder-only (BERT - Hiá»ƒu ngá»¯ cáº£nh), Decoder-only (GPT - Sinh vÄƒn báº£n)
- ğŸ’» **CODE**: DÃ¹ng HuggingFace transformers load thá»­ BERT-base.

### Tuáº§n 39-40: Training Tricks & Optimization

_LÃ m sao Ä‘á»ƒ model há»™i tá»¥ tá»‘t hÆ¡n vÃ  trÃ¡nh Overfitting._

#### NgÃ y 128

- ğŸ“˜ **LEARN**: Regularization: Dropout (Táº¯t ngáº«u nhiÃªn nÆ¡-ron), L1/L2 Regularization (Weight decay)
- ğŸ’» **CODE**: ThÃªm Dropout layer vÃ o model vÃ  so sÃ¡nh káº¿t quáº£.

#### NgÃ y 129

- ğŸ“˜ **LEARN**: Normalization: Batch Normalization (Chuáº©n hÃ³a theo batch), Layer Normalization (DÃ¹ng cho RNN/Transformer)
- ğŸ’» **CODE**: ThÃªm BatchNorm2d vÃ o CNN model.

#### NgÃ y 130

- ğŸ“˜ **LEARN**: Learning Rate Scheduling: Learning Rate Decay, Warm-up steps
- ğŸ’» **CODE**: Sá»­ dá»¥ng StepLR scheduler trong PyTorch.

### Tuáº§n X (Bá»• sung): Multimodal AI â€“ Audio, Video & CLIP

_LÃ m quen vá»›i xá»­ lÃ½ Audio/Video vÃ  mÃ´ hÃ¬nh Ä‘a phÆ°Æ¡ng thá»©c (káº¿t há»£p Text + Image), má»Ÿ rá»™ng kháº£ nÄƒng lÃ m dá»± Ã¡n AI thá»±c táº¿._

#### NgÃ y 131

- ğŸ“˜ **LEARN**: Nháº­p mÃ´n xá»­ lÃ½ Audio: sÃ³ng Ã¢m, sampling rate, spectrogram, Mel-spectrogram. ThÆ° viá»‡n librosa/torchaudio.
- ğŸ’» **CODE**: DÃ¹ng librosa Ä‘á»ƒ Ä‘á»c file .wav, hiá»ƒn thá»‹ waveform vÃ  Mel-spectrogram. LÆ°u hÃ¬nh ra file PNG.

#### NgÃ y 132

- ğŸ“˜ **LEARN**: Speech-to-Text (STT): tá»•ng quan mÃ´ hÃ¬nh Whisper (OpenAI) vÃ  cÃ¡c pipeline STT phá»• biáº¿n.
- ğŸ’» **CODE**: DÃ¹ng thÆ° viá»‡n/CLI Whisper (hoáº·c model STT trÃªn HuggingFace) Ä‘á»ƒ chuyá»ƒn 1 file audio ngáº¯n thÃ nh text. So sÃ¡nh cháº¥t lÆ°á»£ng vá»›i transcript chuáº©n.

#### NgÃ y 133

- ğŸ“˜ **LEARN**: Xá»­ lÃ½ Video báº±ng OpenCV + Deep Learning: Ä‘á»c video, trÃ­ch frame, basic object detection/tracking.
- ğŸ’» **CODE**: Viáº¿t script: Ä‘á»c video, trÃ­ch frame má»—i 1s, cháº¡y sáºµn má»™t model object detection (VD: YOLO pre-trained) trÃªn tá»«ng frame, váº½ bounding box vÃ  lÆ°u video output.

#### NgÃ y 134

- ğŸ“˜ **LEARN**: Multimodal Models: kiáº¿n trÃºc CLIP (Contrastive Language-Image Pretraining). Ã tÆ°á»Ÿng embedding chung cho Text & Image.
- ğŸ’» **CODE**: DÃ¹ng CLIP (OpenAI/HF) Ä‘á»ƒ: (1) Encode má»™t list caption vÃ  má»™t list hÃ¬nh; (2) TÃ¬m caption phÃ¹ há»£p nháº¥t cho má»—i hÃ¬nh (image-text retrieval).

#### NgÃ y 135

- ğŸ“˜ **LEARN**: Thiáº¿t káº¿ bÃ i toÃ¡n Multimodal thá»±c táº¿: recommendation dá»±a trÃªn cáº£ áº£nh + mÃ´ táº£, tÃ¬m kiáº¿m hÃ¬nh áº£nh báº±ng cÃ¢u tá»± nhiÃªn.
- ğŸ’» **CODE**: Mini Project: XÃ¢y má»™t demo simple search â€“ nháº­p cÃ¢u tiáº¿ng Viá»‡t/Anh, dÃ¹ng CLIP Ä‘á»ƒ tÃ¬m ra top-k hÃ¬nh áº£nh phÃ¹ há»£p nháº¥t trong má»™t thÆ° viá»‡n áº£nh nhá». Viáº¿t README mÃ´ táº£ kiáº¿n trÃºc.

### Tuáº§n 41-42: CAPSTONE PROJECT: Multimodal Content Search & Understanding Platform

_XÃ¢y dá»±ng ná»n táº£ng tÃ¬m kiáº¿m ná»™i dung Ä‘a phÆ°Æ¡ng thá»©c, cÃ³ thá»ƒ xá»­ lÃ½ vÃ  tÃ­ch há»£p Text, Image, Audio, Video, tá»« Ä‘Ã³ nÃ¢ng cao kháº£ nÄƒng lÃ m dá»± Ã¡n lá»›n vÃ  thá»±c chiáº¿n._

#### NgÃ y 136

- ğŸ’» **DEFINE**: XÃ¡c Ä‘á»‹nh bÃ i toÃ¡n cá»¥ thá»ƒ (vÃ­ dá»¥: Visual Search, Video Analysis, Content Recommendation hoáº·c Interview Analysis).
- ğŸ’» **COLLECT_DATA**: Thu tháº­p dataset phÃ¹ há»£p (Ã­t nháº¥t 300-500 items), cÃ³ thá»ƒ dÃ¹ng dá»¯ liá»‡u má»Ÿ hoáº·c tá»± táº¡o.

#### NgÃ y 137

- ğŸ’» **PREPROCESS**: Tiá»n xá»­ lÃ½ dá»¯ liá»‡u: chuyá»ƒn Ä‘á»•i audio thÃ nh spectrogram, trÃ­ch xuáº¥t áº£nh, chuáº©n bá»‹ text.
- ğŸ’» **EMBED**: Encode dá»¯ liá»‡u báº±ng cÃ¡c mÃ´ hÃ¬nh phÃ¹ há»£p: CLIP cho áº£nh + text, Wav2Vec2 cho audio, ViT cho áº£nh, speech models cho audio.

#### NgÃ y 138

- ğŸ’» **BUILD_MODEL**: Huáº¥n luyá»‡n hoáº·c fine-tune mÃ´ hÃ¬nh multimodal fusion (vÃ­ dá»¥: dÃ¹ng CLIP hoáº·c mÃ´ hÃ¬nh fusion custom).

#### NgÃ y 139

- ğŸ’» **EVALUATE**: ÄÃ¡nh giÃ¡ há»‡ thá»‘ng: recall@K, precision, visualization báº±ng t-SNE hoáº·c UMAP.

#### NgÃ y 140

- ğŸ’» **DEPLOY**: XÃ¢y dá»±ng API (FastAPI), Ä‘Ã³ng gÃ³i báº±ng Docker, triá»ƒn khai thá»­ trÃªn cloud hoáº·c local.

#### NgÃ y 141

- ğŸ’» **DOCUMENT**: Viáº¿t tÃ i liá»‡u hÆ°á»›ng dáº«n, design architecture, káº¿t quáº£, vÃ  demo trá»±c tuyáº¿n (vÃ­ dá»¥: Streamlit).

## Giai Ä‘oáº¡n 5: LLM, RAG & AI Agents (Generative AI) - Updated

**Thá»i gian:** Tuáº§n 43 Ä‘áº¿n Tuáº§n 62 (20 Tuáº§n)
**Má»¥c tiÃªu:** LÃ m chá»§ cÃ´ng nghá»‡ Generative AI: Tá»« RAG nÃ¢ng cao, GraphRAG Ä‘áº¿n Fine-tuning vÃ  Multi-Agent Systems.
**CÃ´ng cá»¥:** LangChain, LangGraph, LlamaIndex, ChromaDB / Qdrant, Neo4j (Graph DB), HuggingFace PEFT (LoRA), Ollama (Local LLM), Ragas (Evaluation)

### Tuáº§n 43-44: LLM Fundamentals & Structured Output

_Hiá»ƒu cÃ¡ch giao tiáº¿p vÃ  Ä‘iá»u khiá»ƒn mÃ´ hÃ¬nh tráº£ vá» dá»¯ liá»‡u cÃ³ cáº¥u trÃºc._

#### NgÃ y 148

- ğŸ“˜ **LEARN**: LLM Architecture Recap: Pre-training vs Fine-tuning, Context Window & Tokens (CÃ¡ch tÃ­nh tiá»n), Temperature, Top-P (Tham sá»‘ sinh vÄƒn báº£n)
- ğŸ“˜ **LEARN**: Tokenization 101: BPE/SentencePiece, token vs word, OOV; vÃ¬ sao token count áº£nh hÆ°á»Ÿng chi phÃ­ vÃ  giá»›i háº¡n context.
- ğŸ’» **CODE**: Sá»­ dá»¥ng Tiktoken Ä‘á»ƒ Ä‘áº¿m sá»‘ token cá»§a Ä‘oáº¡n vÄƒn.

#### NgÃ y 149

- ğŸ“˜ **LEARN**: Structured Output (JSON Mode): Táº¡i sao LLM cáº§n tráº£ vá» JSON?, Function Calling Ä‘á»ƒ Ã©p kiá»ƒu dá»¯ liá»‡u, ThÆ° viá»‡n Instructor hoáº·c Pydantic OutputParser
- ğŸ“˜ **LEARN**: Attention & Positional Encoding (trá»±c giÃ¡c): vÃ¬ sao transformer 'nhá»›' Ä‘Æ°á»£c ngá»¯ cáº£nh; context window vÃ  KV cache (high-level).
- ğŸ’» **CODE**: Viáº¿t prompt Ã©p model trÃ­ch xuáº¥t thÃ´ng tin tá»« CV ra file JSON Ä‘Ãºng format.

#### NgÃ y 150

- ğŸ“˜ **LEARN**: Running Local LLMs: CÃ i Ä‘áº·t Ollama / LM Studio, Quantization (GGUF) - Táº¡i sao cháº¡y Ä‘Æ°á»£c Llama 3 trÃªn laptop?
- ğŸ“˜ **LEARN**: Decoding Strategies: temperature/top-k/top-p, repetition penalty, stop sequences; deterministic vs sampling.
- ğŸ“˜ **LEARN**: LLM Security intro: Prompt Injection patterns, data exfiltration risks; nguyÃªn táº¯c system prompt, input/output filtering cÆ¡ báº£n.
- ğŸ’» **CODE**: Viáº¿t Python script gá»i API tá»›i localhost Ollama.

### Tuáº§n 45-47: RAG Foundation (Retrieval Augmented Generation)

_Cho LLM 'há»c' dá»¯ liá»‡u riÃªng cá»§a báº¡n mÃ  khÃ´ng cáº§n train láº¡i._

#### NgÃ y 151

- ğŸ“˜ **LEARN**: Vector Embeddings: Biáº¿n vÄƒn báº£n thÃ nh Vector sá»‘ thá»±c, Cosine Similarity (Äo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng), MÃ´ hÃ¬nh Embedding (OpenAI text-embedding-3 vs BGE-M3)
- ğŸ’» **CODE**: Viáº¿t hÃ m tÃ¬m kiáº¿m ngá»¯ nghÄ©a (Semantic Search) Ä‘Æ¡n giáº£n.

#### NgÃ y 152

- ğŸ“˜ **LEARN**: Vector Databases: Cáº¥u trÃºc Vector DB (ChromaDB, Weaviate), CRUD operations trÃªn Vector DB
- ğŸ’» **CODE**: LÆ°u 100 trang tÃ i liá»‡u PDF vÃ o ChromaDB.

#### NgÃ y 153

- ğŸ“˜ **LEARN**: RAG Pipeline cÆ¡ báº£n: Quy trÃ¬nh: Load -> Split -> Embed -> Store -> Retrieve -> Generate, Chunking Strategies (Cáº¯t nhá» vÄƒn báº£n tháº¿ nÃ o cho Ä‘Ãºng?)
- ğŸ’» **CODE**: XÃ¢y dá»±ng Chatbot há»i Ä‘Ã¡p tÃ i liá»‡u Ä‘Æ¡n giáº£n báº±ng LangChain.

### Tuáº§n 48-50: Advanced RAG & GraphRAG

_Kháº¯c phá»¥c nhÆ°á»£c Ä‘iá»ƒm cá»§a RAG cÆ¡ báº£n báº±ng Semantic + Knowledge Graph._

#### NgÃ y 154

- ğŸ“˜ **LEARN**: Hybrid Search: Káº¿t há»£p Keyword Search (BM25) + Vector Search, Táº¡i sao Vector Search tháº¥t báº¡i vá»›i tá»« khÃ³a chÃ­nh xÃ¡c?
- ğŸ’» **CODE**: Implement Hybrid Search dÃ¹ng Qdrant hoáº·c Weaviate.

#### NgÃ y 155

- ğŸ“˜ **LEARN**: Re-ranking (Sáº¯p xáº¿p láº¡i): Cross-Encoder models (Cohere Rerank / BGE-Reranker), Lá»c káº¿t quáº£ rÃ¡c trÆ°á»›c khi gá»­i cho LLM
- ğŸ’» **CODE**: ThÃªm bÆ°á»›c Re-ranking vÃ o pipeline RAG Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c.

#### NgÃ y 156

- ğŸ“˜ **LEARN**: GraphRAG (Knowledge Graph RAG): Háº¡n cháº¿ cá»§a Vector Search (Máº¥t má»‘i quan há»‡ thá»±c thá»ƒ), Knowledge Graph lÃ  gÃ¬? (Neo4j), Káº¿t há»£p Graph + Vector
- ğŸ’» **CODE**: DÃ¹ng LlamaIndex hoáº·c Microsoft GraphRAG Ä‘á»ƒ truy váº¥n má»‘i quan há»‡ phá»©c táº¡p.

### Tuáº§n 51-53: LangChain & LlamaIndex Deep Dive

_ThÃ nh tháº¡o framework phÃ¡t triá»ƒn á»©ng dá»¥ng LLM sá»‘ 1 hiá»‡n nay._

#### NgÃ y 157

- ğŸ“˜ **LEARN**: LangChain LCEL: LangChain Expression Language (Pipe syntax | ), Runnables & Chains
- ğŸ’» **CODE**: Viáº¿t Chain: Prompt | Model | OutputParser.

#### NgÃ y 158

- ğŸ“˜ **LEARN**: Memory & History: Quáº£n lÃ½ lá»‹ch sá»­ chat (ConversationBufferMemory), LÆ°u history vÃ o Redis/Postgres
- ğŸ’» **CODE**: Táº¡o Chatbot nhá»› Ä‘Æ°á»£c tÃªn ngÆ°á»i dÃ¹ng qua nhiá»u lÆ°á»£t chat.

#### NgÃ y 159

- ğŸ“˜ **LEARN**: LlamaIndex for Data: Data Connectors (Láº¥y dá»¯ liá»‡u tá»« Notion, Slack, SQL), LlamaIndex Query Engine
- ğŸ’» **CODE**: XÃ¢y dá»±ng há»‡ thá»‘ng há»i Ä‘Ã¡p trÃªn cÆ¡ sá»Ÿ dá»¯ liá»‡u SQL (Text-to-SQL).

### Tuáº§n 54-56: AI Agents & Orchestration (LangGraph)

_XÃ¢y dá»±ng AI chá»§ Ä‘á»™ng thá»±c hiá»‡n hÃ nh Ä‘á»™ng (TÆ°Æ¡ng lai cá»§a AI)._

#### NgÃ y 160

- ğŸ“˜ **LEARN**: Tool Calling (Function Calling): Dáº¡y LLM cÃ¡ch dÃ¹ng cÃ´ng cá»¥ (Calculator, Google Search API), Äá»‹nh nghÄ©a Tools báº±ng Pydantic
- ğŸ’» **CODE**: Táº¡o Agent biáº¿t tá»± tÃ­nh toÃ¡n vÃ  search web.

#### NgÃ y 161

- ğŸ“˜ **LEARN**: LangGraph Basics: Stateful Agents (Agent cÃ³ tráº¡ng thÃ¡i), Nodes & Edges (Quy trÃ¬nh dáº¡ng Ä‘á»“ thá»‹), Cyclic Graphs (VÃ²ng láº·p suy nghÄ©)
- ğŸ’» **CODE**: XÃ¢y dá»±ng luá»“ng Agent: Plan -> Execute -> Reflect (Tá»± kiá»ƒm tra) -> Output.

#### NgÃ y 162

- ğŸ“˜ **LEARN**: Multi-Agent Systems: PhÃ¢n chia nhiá»‡m vá»¥: Researcher Agent & Writer Agent, Supervisor Agent (Quáº£n lÃ½ chung)
- ğŸ’» **CODE**: XÃ¢y dá»±ng Ä‘á»™i ngÅ© Agent tá»± Ä‘á»™ng viáº¿t bÃ i blog nghiÃªn cá»©u.

### Tuáº§n 57-58: LLM Evaluation (Kiá»ƒm thá»­ cháº¥t lÆ°á»£ng)

_LÃ m sao biáº¿t Chatbot tráº£ lá»i Ä‘Ãºng hay sai?_

#### NgÃ y 163

- ğŸ“˜ **LEARN**: RAG Evaluation Metrics: Faithfulness (Trung thá»±c vá»›i dá»¯ liá»‡u nguá»“n), Answer Relevance (Tráº£ lá»i Ä‘Ãºng trá»ng tÃ¢m), Context Recall (TÃ¬m Ä‘á»§ thÃ´ng tin khÃ´ng)
- ğŸ’» **CODE**: DÃ¹ng thÆ° viá»‡n Ragas Ä‘á»ƒ cháº¥m Ä‘iá»ƒm há»‡ thá»‘ng RAG.

#### NgÃ y 164

- ğŸ“˜ **LEARN**: Observability (Quan sÃ¡t): Tracing vá»›i LangSmith hoáº·c Arize Phoenix, Debug tá»«ng bÆ°á»›c cháº¡y cá»§a Chain
- ğŸ’» **CODE**: TÃ­ch há»£p LangSmith Ä‘á»ƒ theo dÃµi token usage vÃ  latency.

### Tuáº§n 59-60: Fine-tuning & Optimization (NÃ¢ng cao)

_TÃ¹y biáº¿n Model cho tÃ¡c vá»¥ chuyÃªn biá»‡t._

#### NgÃ y 165

- ğŸ“˜ **LEARN**: PEFT & LoRA: Parameter-Efficient Fine-Tuning (Chá»‰ train < 1% tham sá»‘), Low-Rank Adaptation (LoRA)
- ğŸ’» **CODE**: Chuáº©n bá»‹ dataset Ä‘á»‹nh dáº¡ng JSONL cho fine-tuning.

#### NgÃ y 166

- ğŸ“˜ **LEARN**: Fine-tuning & Serving Optimization: Sá»­ dá»¥ng thÆ° viá»‡n Unsloth (Tá»‘i Æ°u tá»‘c Ä‘á»™ train x2), Quantization for Serving (AWQ / GPTQ) Ä‘á»ƒ giáº£m chi phÃ­ VRAM
- ğŸ’» **CODE**: Fine-tune model Llama-3 vÃ  export ra Ä‘á»‹nh dáº¡ng GGUF Ä‘á»ƒ cháº¡y tiáº¿t kiá»‡m.

### Tuáº§n Y (Bá»• sung): Advanced LLM Training â€“ RLHF & DPO

_Hiá»ƒu vÃ  thá»±c hÃ nh cÃ¡c ká»¹ thuáº­t tá»‘i Æ°u LLM hiá»‡n Ä‘áº¡i dá»±a trÃªn pháº£n há»“i con ngÆ°á»i: RLHF vÃ  DPO._

#### NgÃ y 167

- ğŸ“˜ **LEARN**: Tá»•ng quan RLHF (Reinforcement Learning from Human Feedback): pipeline 3 bÆ°á»›c â€“ (1) SFT, (2) Reward Model, (3) RL (PPO).
- ğŸ“˜ **LEARN**: Äá»c tÃ³m táº¯t paper InstructGPT hoáº·c RLHF overview, ghi chÃº láº¡i flow dá»¯ liá»‡u vÃ  má»¥c tiÃªu cá»§a tá»«ng bÆ°á»›c.

#### NgÃ y 168

- ğŸ“˜ **LEARN**: Reward Model: cÃ¡ch huáº¥n luyá»‡n model phÃ¢n biá»‡t giá»¯a cÃ¢u tráº£ lá»i tá»‘t/xáº¥u dá»±a trÃªn cáº·p preference (A tá»‘t hÆ¡n B).
- ğŸ’» **CODE**: Chuáº©n bá»‹ má»™t mini-dataset gá»“m cÃ¡c cáº·p (prompt, answer_good, answer_bad) dáº¡ng JSON/CSV Ä‘á»ƒ dÃ¹ng cho reward model hoáº·c DPO.

#### NgÃ y 169

- ğŸ“˜ **LEARN**: Giá»›i thiá»‡u thÆ° viá»‡n TRL (HuggingFace) â€“ há»— trá»£ PPO, DPO. So sÃ¡nh high-level RLHF vs DPO (Direct Preference Optimization).
- ğŸ’» **CODE**: DÃ¹ng TRL hoáº·c thÆ° viá»‡n tÆ°Æ¡ng tá»± Ä‘á»ƒ cháº¡y thá»­ má»™t vÃ²ng PPO ráº¥t nhá» trÃªn model nhá» (VD: distilGPT2) vá»›i 5â€“10 máº«u preference (demo).

#### NgÃ y 170

- ğŸ“˜ **LEARN**: DPO (Direct Preference Optimization): Ä‘á»™ng lá»±c ra Ä‘á»i, Æ°u Ä‘iá»ƒm so vá»›i RLHF cá»• Ä‘iá»ƒn (Ä‘Æ¡n giáº£n hÆ¡n, á»•n Ä‘á»‹nh hÆ¡n).
- ğŸ’» **CODE**: Ãp dá»¥ng DPO trÃªn chÃ­nh dataset preference nhá» á»Ÿ trÃªn Ä‘á»ƒ tinh chá»‰nh má»™t model nhá». So sÃ¡nh output trÆ°á»›c/sau DPO trÃªn vÃ i prompt.

#### NgÃ y 171

- ğŸ“˜ **LEARN**: CÃ¡c váº¥n Ä‘á» thá»±c táº¿ cá»§a RLHF/DPO: cháº¥t lÆ°á»£ng dá»¯ liá»‡u human feedback, bias, over-optimization, chi phÃ­ compute.
- ğŸ’» **CODE**: Viáº¿t bÃ¡o cÃ¡o ngáº¯n (Markdown/Jupyter): mÃ´ táº£ pipeline RLHF/DPO báº¡n Ä‘Ã£ thá»­, cÃ¡c háº¡n cháº¿ do dataset nhá», vÃ  káº¿ hoáº¡ch má»Ÿ rá»™ng náº¿u cÃ³ GPU + dá»¯ liá»‡u tháº­t.

### Tuáº§n 61-62: CAPSTONE PROJECT GIAI ÄOáº N 5

#### NgÃ y 172

- ğŸ“˜ **LEARN**: Project: Intelligent Legal/Medical Assistant (Agentic RAG)
- ğŸ’» **CODE**: 1. Data Pipeline: Crawl vÃ  xá»­ lÃ½ sáº¡ch dá»¯ liá»‡u vÄƒn báº£n phÃ¡p luáº­t/y khoa.
- ğŸ’» **CODE**: 2. Hybrid RAG: Káº¿t há»£p Vector Search + Keyword Search + Re-ranking.
- ğŸ’» **CODE**: 3. GraphRAG (Optional): XÃ¢y dá»±ng Knowledge Graph cho cÃ¡c Ä‘iá»u luáº­t liÃªn quan nhau.
- ğŸ’» **CODE**: 4. Agent Workflow (LangGraph): Agent cÃ³ kháº£ nÄƒng há»i láº¡i user náº¿u thiáº¿u thÃ´ng tin (Human-in-the-loop).
- ğŸ’» **CODE**: 5. Evaluation: Viáº¿t bá»™ test case gá»“m 50 cÃ¢u há»i khÃ³ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c.
- ğŸ’» **CODE**: 6. UI: Giao diá»‡n Chat chuyÃªn nghiá»‡p (Streamlit/Chainlit).

## Giai Ä‘oáº¡n 6: Software Engineering & Data Ops (Updated)

**Thá»i gian:** Tuáº§n 63 Ä‘áº¿n Tuáº§n 74 (12 Tuáº§n)
**Má»¥c tiÃªu:** XÃ¢y dá»±ng Backend vá»¯ng cháº¯c, báº£o máº­t, Ä‘Ã³ng gÃ³i á»©ng dá»¥ng tá»‘i Æ°u vÃ  tá»± Ä‘á»™ng hÃ³a quy trÃ¬nh (CI/CD).
**CÃ´ng cá»¥:** FastAPI, Uvicorn/Gunicorn, Docker & Docker Compose, Kubernetes (K8s Basic), GitHub Actions, PostgreSQL (pgvector), Redis, Celery / RabbitMQ

### Tuáº§n 63-64: Backend API Development & Security

_Viáº¿t API hiá»‡u nÄƒng cao, báº£o máº­t vÃ  chuáº©n chá»‰nh._

#### NgÃ y 173

- ğŸ“˜ **LEARN**: FastAPI, Pydantic & Config Management: Pydantic Settings (Quáº£n lÃ½ biáº¿n mÃ´i trÆ°á»ng .env an toÃ n), Dependency Injection trong FastAPI (Clean Architecture), Middleware (CORS, Logging)
- ğŸ’» **CODE**: Viáº¿t API load config tá»« file .env vÃ  log má»i request vÃ o file.

#### NgÃ y 174

- ğŸ“˜ **LEARN**: Authentication & Security: OAuth2 vá»›i Password Flow, JWT (JSON Web Tokens) - Táº¡o vÃ  xÃ¡c thá»±c token, Rate Limiting (Chá»‘ng DDOS Ä‘Æ¡n giáº£n)
- ğŸ’» **CODE**: TÃ­ch há»£p Ä‘Äƒng nháº­p JWT vÃ o API, báº£o vá»‡ route /admin.

#### NgÃ y 175

- ğŸ“˜ **LEARN**: Asyncio & Streaming Response: Server-Sent Events (SSE) cho Chatbot, Websockets (Giao tiáº¿p 2 chiá»u realtime), Xá»­ lÃ½ tÃ¡c vá»¥ ná»n (Background Tasks)
- ğŸ’» **CODE**: Táº¡o API chat tráº£ vá» tá»«ng token (Streaming) giá»‘ng ChatGPT.

### Tuáº§n 65-66: Containerization (Docker Advanced)

_Tá»‘i Æ°u hÃ³a Docker Image cho mÃ´i trÆ°á»ng Production._

#### NgÃ y 176

- ğŸ“˜ **LEARN**: Docker Best Practices: Multi-stage builds (Giáº£m size image tá»« 1GB -> 200MB), .dockerignore (TrÃ¡nh copy file rÃ¡c), Non-root user (Báº£o máº­t container)
- ğŸ’» **CODE**: Viáº¿t Dockerfile multi-stage tá»‘i Æ°u cho á»©ng dá»¥ng Python.

#### NgÃ y 177

- ğŸ“˜ **LEARN**: Docker for AI/GPU: NVIDIA Container Toolkit (Cháº¡y model trÃªn GPU docker), Caching pip packages Ä‘á»ƒ build nhanh hÆ¡n
- ğŸ’» **CODE**: Build image Docker há»— trá»£ CUDA Ä‘á»ƒ cháº¡y PyTorch.

#### NgÃ y 178

- ğŸ“˜ **LEARN**: Docker Compose for Dev Environment: Orchestrate API + DB + Redis + Worker, Healthchecks (Äá»£i DB khá»Ÿi Ä‘á»™ng xong má»›i cháº¡y API)
- ğŸ’» **CODE**: Setup docker-compose.yml cháº¡y full stack local.

### Tuáº§n 67-68: Database & Async Workers

_Xá»­ lÃ½ dá»¯ liá»‡u lá»›n vÃ  tÃ¡c vá»¥ náº·ng khÃ´ng lÃ m treo API._

#### NgÃ y 179

- ğŸ“˜ **LEARN**: PostgreSQL & pgvector: Thiáº¿t káº¿ Schema tá»‘i Æ°u cho Vector Search, Indexing (HNSW index) Ä‘á»ƒ tÃ¬m kiáº¿m nhanh, Migrations vá»›i Alembic (Quáº£n lÃ½ thay Ä‘á»•i DB)
- ğŸ’» **CODE**: Táº¡o báº£ng lÆ°u Embeddings vÃ  query tÃ¬m kiáº¿m vector tÆ°Æ¡ng Ä‘á»“ng.

#### NgÃ y 180

- ğŸ“˜ **LEARN**: Redis & Async Workers (Celery): Redis Caching strategy, Message Queue (RabbitMQ/Redis), Celery (Xá»­ lÃ½ tÃ¡c vá»¥ náº·ng nhÆ° parse PDF, train model)
- ğŸ’» **CODE**: Setup Worker xá»­ lÃ½ viá»‡c upload file PDF á»Ÿ background.

### Tuáº§n 69-70: Testing & CI/CD

_Tá»± Ä‘á»™ng hÃ³a quy trÃ¬nh kiá»ƒm thá»­ vÃ  deploy._

#### NgÃ y 181

- ğŸ“˜ **LEARN**: Automated Testing (Pytest): Unit Test vs Integration Test, Test Client (FastAPI) & Fixtures, Mocking External APIs (KhÃ´ng gá»i OpenAI tháº­t khi test)
- ğŸ’» **CODE**: Viáº¿t test case coverage > 80% cho module Auth.

#### NgÃ y 182

- ğŸ“˜ **LEARN**: GitHub Actions (CI/CD): Pipeline: Lint -> Test -> Build -> Push Docker Hub, Quáº£n lÃ½ Secrets trÃªn GitHub
- ğŸ’» **CODE**: Setup workflow tá»± Ä‘á»™ng build image khi push vÃ o nhÃ¡nh main.

### Tuáº§n 71-72: Orchestration Intro (Kubernetes)

_Hiá»ƒu khÃ¡i niá»‡m Ä‘á»ƒ deploy lÃªn Cloud (AWS EKS / GKE)._

#### NgÃ y 183

- ğŸ“˜ **LEARN**: Kubernetes Concepts: Pod, Deployment, Service, Ingress, ConfigMap & Secrets (Quáº£n lÃ½ cáº¥u hÃ¬nh K8s)
- ğŸ’» **CODE**: Viáº¿t manifest deployment.yaml vÃ  service.yaml.

#### NgÃ y 184

- ğŸ“˜ **LEARN**: Deploying & Scaling: Horizontal Pod Autoscaling (HPA), Rolling Updates (Deploy khÃ´ng downtime)
- ğŸ’» **CODE**: Deploy á»©ng dá»¥ng lÃªn cá»¥m K8s local (Minikube).

### Tuáº§n 73-74: CAPSTONE PROJECT GIAI ÄOáº N 6

#### NgÃ y 185

- ğŸ“˜ **LEARN**: Project: Production-Grade GenAI Backend
- ğŸ’» **CODE**: 1. Security: TÃ­ch há»£p JWT Auth, Rate Limiting, CORS.
- ğŸ’» **CODE**: 2. Async Architecture: API nháº­n request -> Ä‘áº©y vÃ o Queue -> Worker xá»­ lÃ½ -> API tráº£ káº¿t quáº£ (hoáº·c Webhook).
- ğŸ’» **CODE**: 3. Database: Postgres (User/Data) + Qdrant (Vector) + Redis (Cache/Queue).
- ğŸ’» **CODE**: 4. Docker: Multi-stage build, image size < 500MB (náº¿u khÃ´ng kÃ¨m model) hoáº·c tá»‘i Æ°u layer model.
- ğŸ’» **CODE**: 5. CI/CD: Pipeline test vÃ  build tá»± Ä‘á»™ng.
- ğŸ’» **CODE**: 6. Monitoring: TÃ­ch há»£p Prometheus/Grafana cÆ¡ báº£n Ä‘á»ƒ Ä‘o API latency.

## Giai Ä‘oáº¡n 7: MLOps & Production Optimization (Updated)

**Thá»i gian:** Tuáº§n 75 Ä‘áº¿n Tuáº§n 86 (12 Tuáº§n)
**Má»¥c tiÃªu:** Triá»ƒn khai model chuáº©n chá»‰nh (IaC), tá»‘i Æ°u chi phÃ­ vÃ  giÃ¡m sÃ¡t an toÃ n há»‡ thá»‘ng.
**CÃ´ng cá»¥:** AWS (EC2, SageMaker, Lambda), Terraform / AWS CDK (Infrastructure as Code), vLLM / Triton Server, MLflow, Prometheus & Grafana, NVIDIA NeMo Guardrails (AI Security)

### Tuáº§n 75-76: Model Serving Optimization

_Tá»‘i Æ°u hÃ³a Ä‘á»™ trá»… (Latency) vÃ  thÃ´ng lÆ°á»£ng (Throughput) cho LLM._

#### NgÃ y 186

- ğŸ“˜ **LEARN**: Inference Engines (vLLM), PagedAttention & Continuous Batching, Thiáº¿t láº­p vLLM server cho Llama-3/Mistral
- ğŸ’» **CODE**: Triá»ƒn khai vLLM server dockerized, expose API chuáº©n OpenAI.

#### NgÃ y 187

- ğŸ“˜ **LEARN**: Triton Inference Server, Kiáº¿n trÃºc NVIDIA Triton, Model Ensemble (Káº¿t há»£p nhiá»u model: Preprocess -> Model -> Postprocess)
- ğŸ’» **CODE**: Cáº¥u hÃ¬nh model repository cho Triton cháº¡y model ONNX.

#### NgÃ y 188

- ğŸ“˜ **LEARN**: Load Testing (Stress Test), Locust / JMeter, Äo Ä‘áº¡c RPS (Requests per second) vÃ  P99 Latency
- ğŸ’» **CODE**: Viáº¿t script Locust giáº£ láº­p 1000 user chat cÃ¹ng lÃºc Ä‘á»ƒ tÃ¬m Ä‘iá»ƒm cháº¿t cá»§a server.

### Tuáº§n 77-78: Quantization & Cost Optimization

_Cháº¡y model lá»›n trÃªn pháº§n cá»©ng ráº» tiá»n._

#### NgÃ y 189

- ğŸ“˜ **LEARN**: Quantization Techniques, GGUF (CPU Inference), AWQ / GPTQ (GPU Inference tá»‘i Æ°u), KV Cache Quantization
- ğŸ’» **CODE**: Convert model Llama-3-8B FP16 sang AWQ 4-bit giÃºp giáº£m 60% VRAM.

#### NgÃ y 190

- ğŸ“˜ **LEARN**: Distillation & Pruning, Teacher-Student Training (Model nhá» há»c tá»« model to), Structural Pruning (Cáº¯t bá» nÆ¡-ron thá»«a)
- ğŸ’» **CODE**: Thá»±c hÃ nh Distillation: Dáº¡y model TinyLlama há»c theo GPT-4.

### Tuáº§n 79-81: Cloud Infrastructure & IaC (ChuyÃªn nghiá»‡p)

_Quáº£n lÃ½ háº¡ táº§ng báº±ng Code (KhÃ´ng click tay)._

#### NgÃ y 191

- ğŸ“˜ **LEARN**: AWS Compute for AI, EC2 G4dn/G5 instances (NVIDIA GPU), AWS Spot Instances (Bid giÃ¡ ráº»), Deep Learning AMI (Setup sáºµn driver)
- ğŸ’» **CODE**: Launch má»™t Spot Instance giÃ¡ ráº» Ä‘á»ƒ train model.

#### NgÃ y 192

- ğŸ“˜ **LEARN**: Infrastructure as Code (IaC), KhÃ¡i niá»‡m IaC (Terraform hoáº·c AWS CDK), Táº¡i sao khÃ´ng nÃªn config thá»§ cÃ´ng trÃªn Console?
- ğŸ’» **CODE**: Viáº¿t file Terraform Ä‘Æ¡n giáº£n Ä‘á»ƒ tá»± Ä‘á»™ng táº¡o EC2 vÃ  Security Group.

#### NgÃ y 193

- ğŸ“˜ **LEARN**: Serverless Inference, AWS Lambda (Cho model nhá»/CPU), SageMaker Serverless Inference, RunPod / Modal (Alternative providers)
- ğŸ’» **CODE**: Deploy hÃ m xá»­ lÃ½ áº£nh Ä‘Æ¡n giáº£n lÃªn AWS Lambda + EFS.

### Tuáº§n 82-83: MLOps Lifecycle (Tracking & CI/CD)

_Tá»± Ä‘á»™ng hÃ³a quy trÃ¬nh huáº¥n luyá»‡n vÃ  triá»ƒn khai._

#### NgÃ y 194

- ğŸ“˜ **LEARN**: Experiment Tracking (MLflow), Logging metrics/params, Artifact Storage (S3), MLflow UI
- ğŸ’» **CODE**: Setup MLflow Server remote (trÃªn EC2) káº¿t ná»‘i vá»›i S3 bucket.

#### NgÃ y 195

- ğŸ“˜ **LEARN**: Model Registry & CD, Promote model (Staging -> Production), Trigger deploy khi cÃ³ model má»›i
- ğŸ’» **CODE**: GitHub Actions: Tá»± Ä‘á»™ng deploy model khi chuyá»ƒn tráº¡ng thÃ¡i sang 'Production' trong MLflow.

### Tuáº§n 84-85: Monitoring & AI Security (Guardrails)

_GiÃ¡m sÃ¡t sá»©c khá»e vÃ  Ä‘áº£m báº£o an toÃ n cho AI._

#### NgÃ y 196

- ğŸ“˜ **LEARN**: AI Security (Guardrails), Prompt Injection Attacks, Input/Output Filtering (Lá»c ná»™i dung Ä‘á»™c háº¡i/nháº¡y cáº£m), NVIDIA NeMo Guardrails
- ğŸ’» **CODE**: TÃ­ch há»£p Guardrails cháº·n chatbot nÃ³i báº­y hoáº·c tiáº¿t lá»™ thÃ´ng tin cÃ¡ nhÃ¢n.

#### NgÃ y 197

- ğŸ“˜ **LEARN**: System Monitoring, Prometheus (Thu tháº­p metrics), Grafana (Váº½ biá»ƒu Ä‘á»“), Drift Detection (PhÃ¡t hiá»‡n model bá»‹ sai dáº§n theo thá»i gian)
- ğŸ’» **CODE**: Setup Dashboard Grafana cáº£nh bÃ¡o khi GPU > 90% hoáº·c API error rate > 1%.

### Tuáº§n 86: CAPSTONE PROJECT GIAI ÄOáº N 7

_XÃ¢y dá»±ng ná»n táº£ng LLM ná»™i bá»™ hoÃ n chá»‰nh trÃªn AWS._

#### NgÃ y 198

- ğŸ“˜ **LEARN**: Project: Production-Grade LLM Platform
- ğŸ’» **CODE**: 1. Infrastructure: DÃ¹ng Terraform dá»±ng VPC, EC2 (vLLM), RDS (Database).
- ğŸ’» **CODE**: 2. Deployment: Deploy model Llama-3 dáº¡ng Quantized (AWQ) trÃªn Docker.
- ğŸ’» **CODE**: 3. Security: TÃ­ch há»£p NeMo Guardrails cháº·n Prompt Injection.
- ğŸ’» **CODE**: 4. Pipeline: Auto-deploy khi update code má»›i (GitHub Actions).
- ğŸ’» **CODE**: 5. Monitoring: Full dashboard Grafana theo dÃµi token/s, latency vÃ  chi phÃ­ Æ°á»›c tÃ­nh.

## Giai Ä‘oáº¡n 8 (Bá»• sung): ML System Design & Scalable AI Engineering

**Thá»i gian:** 3â€“4 Tuáº§n | Táº­p trung System Design, Distributed Training & Production
**Má»¥c tiÃªu:** XÃ¢y dá»±ng tÆ° duy kiáº¿n trÃºc há»‡ thá»‘ng AI end-to-end, chá»‹u táº£i cao, dá»… má»Ÿ rá»™ng; hiá»ƒu cÃ¡ch train/serve model lá»›n trÃªn nhiá»u GPU/mÃ¡y.
**CÃ´ng cá»¥:** AWS/GCP/Azure (hoáº·c Local + Docker), Kubernetes (cÆ¡ báº£n), Torch DDP/FSDP hoáº·c DeepSpeed, Prometheus/Grafana (hoáº·c tÆ°Æ¡ng Ä‘Æ°Æ¡ng)

### Tuáº§n n: ML System Design Fundamentals

_Náº¯m Ä‘Æ°á»£c cÃ¡c thÃ nh pháº§n chÃ­nh cá»§a má»™t há»‡ thá»‘ng ML end-to-end tá»« Data Ä‘áº¿n Monitoring._

#### NgÃ y 199

- ğŸ“˜ **LEARN**: ML System Lifecycle: Data â†’ ETL â†’ Training â†’ Evaluation â†’ Deployment â†’ Monitoring â†’ Feedback Loop.
- ğŸ’» **CODE**: Váº½ sÆ¡ Ä‘á»“ kiáº¿n trÃºc (báº±ng draw.io, Excalidraw hoáº·c Mermaid) cho má»™t há»‡ thá»‘ng recommendation Ä‘Æ¡n giáº£n (offline batch prediction).

#### NgÃ y 200

- ğŸ“˜ **LEARN**: Batch vs Online vs Streaming Inference: Æ°u/nhÆ°á»£c Ä‘iá»ƒm, vÃ­ dá»¥ use-case cho tá»«ng loáº¡i.
- ğŸ’» **CODE**: Thiáº¿t káº¿ 2 biáº¿n thá»ƒ cho cÃ¹ng má»™t bÃ i toÃ¡n (VD: fraud detection): (1) Batch inference háº±ng ngÃ y; (2) Real-time API. Viáº¿t README so sÃ¡nh.

#### NgÃ y 201

- ğŸ“˜ **LEARN**: Feature Store & Data Contract: quáº£n lÃ½ feature dÃ¹ng chung cho training & serving, Ä‘áº£m báº£o khÃ´ng bá»‹ training-serving skew.
- ğŸ’» **CODE**: Thiáº¿t káº¿ (dáº¡ng JSON/YAML) schema cho má»™t bá»™ feature dÃ¹ng láº¡i Ä‘Æ°á»£c (user_features, item_features, interaction_features) vÃ  mÃ´ táº£ cÃ¡ch cáº­p nháº­t.

#### NgÃ y 202

- ğŸ“˜ **LEARN**: System Design Interview cho ML/AI: cÃ¡ch phÃ¢n tÃ­ch yÃªu cáº§u, váº½ kiáº¿n trÃºc, chá»n trade-off.
- ğŸ’» **CODE**: Viáº¿t má»™t báº£n design doc ngáº¯n (1â€“2 trang) cho há»‡ thá»‘ng AI báº¥t ká»³ báº¡n Ä‘Ã£ lÃ m á»Ÿ Capstone trÆ°á»›c: má»¥c tiÃªu, kiáº¿n trÃºc, trade-off.

### Tuáº§n n: Distributed Training & Large-scale Serving

_Hiá»ƒu cÃ¡ch huáº¥n luyá»‡n vÃ  phá»¥c vá»¥ model lá»›n trÃªn nhiá»u GPU/mÃ¡y, giáº£m lá»—i Out-Of-Memory vÃ  tÄƒng throughput._

#### NgÃ y 203

- ğŸ“˜ **LEARN**: Tá»•ng quan Distributed Training: Data Parallel, Model Parallel, Pipeline Parallel. CÃ¡c thÆ° viá»‡n: PyTorch DDP, FSDP, DeepSpeed.
- ğŸ’» **CODE**: Äá»c vÃ  cháº¡y vÃ­ dá»¥ official PyTorch DDP trÃªn 2 GPU (hoáº·c mÃ´ phá»ng multi-process trÃªn 1 GPU/CPU náº¿u mÃ¡y yáº¿u). Ghi chÃº láº¡i cáº¥u trÃºc code.

#### NgÃ y 204

- ğŸ“˜ **LEARN**: Memory Optimization: gradient checkpointing, mixed precision (FP16/BF16), offloading parameter sang CPU/disk.
- ğŸ’» **CODE**: Láº¥y má»™t model tÆ°Æ¡ng Ä‘á»‘i lá»›n (VD: LLM nhá», Vision Transformer), so sÃ¡nh sá»­ dá»¥ng bá»™ nhá»› khi: (1) FP32 full; (2) mixed precision; (3) gradient checkpointing.

#### NgÃ y 205

- ğŸ“˜ **LEARN**: Model Serving: so sÃ¡nh REST vs gRPC, single model server vs model gateway. KhÃ¡i niá»‡m autoscaling, load balancing.
- ğŸ’» **CODE**: ÄÃ³ng gÃ³i má»™t model Ä‘Ã£ huáº¥n luyá»‡n thÃ nh service (FastAPI/Flask + Docker). Äo latency khi gá»i nhiá»u request song song (dÃ¹ng locust/ab).

#### NgÃ y 206

- ğŸ“˜ **LEARN**: Monitoring & Observability cho há»‡ thá»‘ng AI: metrics (latency, error rate), model metrics (drift, data quality), logging.
- ğŸ’» **CODE**: ThÃªm logging cÆ¡ báº£n vÃ  thu tháº­p metric (VD: báº±ng Prometheus client hoáº·c custom logs) cho service model. Váº½ báº£ng/Ä‘á»“ thá»‹ Ä‘Æ¡n giáº£n tá»« log.

### Tuáº§n nâ€“n+1: Capstone â€“ Thiáº¿t káº¿ & triá»ƒn khai há»‡ thá»‘ng AI lá»›n

_Tá»•ng há»£p toÃ n bá»™ kiáº¿n thá»©c Ä‘á»ƒ thiáº¿t káº¿ má»™t há»‡ thá»‘ng AI gáº§n vá»›i sáº£n pháº©m thá»±c táº¿, cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng._

#### NgÃ y 207

- ğŸ“˜ **LEARN**: Chá»n bÃ i toÃ¡n: recommendation, search, chatbot LLM, hoáº·c fraud detectionâ€¦ theo sá»Ÿ thÃ­ch cá»§a báº¡n.
- ğŸ’» **CODE**: Viáº¿t Problem Statement & Requirement rÃµ rÃ ng: loáº¡i data, SLA (latency, throughput), constraint (ngÃ¢n sÃ¡ch, sá»‘ GPU).

#### NgÃ y 208

- ğŸ’» **CODE**: Thiáº¿t káº¿ kiáº¿n trÃºc end-to-end chi tiáº¿t: (1) Data pipeline (batch/streaming), (2) Training pipeline, (3) Serving architecture, (4) Monitoring. Váº½ sÆ¡ Ä‘á»“, mÃ´ táº£ tá»«ng component.

#### NgÃ y 209

- ğŸ’» **CODE**: Triá»ƒn khai má»™t báº£n MVP: dá»¯ liá»‡u nhá» + 1â€“2 microservice (model API, data preprocessor). DÃ¹ng Docker Ä‘á»ƒ cháº¡y toÃ n bá»™ trÃªn cÃ¹ng má»™t mÃ¡y hoáº·c cloud ráº».

#### NgÃ y 210

- ğŸ’» **CODE**: Thá»±c hiá»‡n load test nhá» (giáº£ láº­p 100â€“1000 request/phÃºt tÃ¹y tÃ i nguyÃªn), ghi láº¡i káº¿t quáº£, bottleneck, vÃ  Ä‘á» xuáº¥t hÆ°á»›ng tá»‘i Æ°u (scale up/out, cache, batching). Viáº¿t Design Doc/Report hoÃ n chá»‰nh Ä‘Æ°a lÃªn GitHub.
